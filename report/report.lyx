#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\usepackage[font=scriptsize]{caption}
\captionsetup[figure]{font=scriptsize}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "palatino" "EB Garamond"
\font_sans "helvet" "Arial"
\font_typewriter "courier" "Courier"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing onehalf
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 2cm
\headsep 2cm
\footskip 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Does forgetting save the brain's metabolic energy for future learning?
\end_layout

\begin_layout Author
Pooja Nagendra Babu (20216094)
\end_layout

\begin_layout Subsection*
Forgetting
\end_layout

\begin_layout Standard
The process of learning in the brain encodes information as memories which
 helps us to adapt to the surroundings and makes us who we are.
 Traditionally, forgetting in the brain is regarded as a slow and natural
 decay of memories over time due to the general instability of biological
 mechanisms.
 This form of forgetting is referred to as 
\emph on
passive forgetting
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "DAVIS2017490"
literal "false"

\end_inset

.
 This may be due to the loss of context cues over time or interference due
 to other similar memories.
 This is translated to decay in weights over time in the context of neural
 networks.
 This kind of decay can improve generalization by suppressing irrelevant
 components of the weight vector and can suppress some effects of static
 noise on the targets 
\begin_inset CommandInset citation
LatexCommand cite
key "NIPS1991_563"
literal "false"

\end_inset

.
 It is also established that a Hebbian Learning rule with passive forgetting
 in a chaotic neural network (CNN) acts as a fuzzy-like pattern classifier
 that performs better than the ordinary CNN 
\begin_inset CommandInset citation
LatexCommand cite
key "LI2012412"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Learning can also happen continually where the networks learn by accommodating
 knowledge over time by learning a sequence of tasks.
 This process called continual learning is the ability to learn consecutive
 tasks without forgetting how to perform the previously learned tasks 
\begin_inset CommandInset citation
LatexCommand cite
key "Kirkpatrick3521"
literal "false"

\end_inset

.
 This poses a problem for the networks to learn a sequence of tasks where
 learning new information may degrade the performance due to the information
 loss of the previously learned tasks.
 This property of the networks to forget the old information when learning
 the new information is called 
\emph on
catastrophic forgetting
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "MCCLOSKEY1989109"
literal "false"

\end_inset

, which can be considered as a form of 
\emph on
active forgetting
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "DAVIS2017490"
literal "false"

\end_inset

.
 This can lead to a trade-off between the extent to which the system can
 become plastic in order to learn new information and stable in order to
 not catastrophically forget the old acquired knowledge, often referred
 to as the stability-plasticity dilemma 
\begin_inset CommandInset citation
LatexCommand cite
key "ABRAHAM200573"
literal "false"

\end_inset

.
 In contrast, humans and other animals can learn in a continual fashion
 in a lifelong manner.
 There are experimental evidence to suggest that in order to prevent catastrophi
c forgetting, there are specialized systems in the hippocampus that allows
 for learning of new information which, over time, will be transferred to
 the neocortical system for long-term storage 
\begin_inset CommandInset citation
LatexCommand cite
key "PARISI201954"
literal "false"

\end_inset

.
\end_layout

\begin_layout Section*
Results
\end_layout

\begin_layout Subsection*
Learning through perceptron
\end_layout

\begin_layout Standard
The metabolic energy expended due to synaptic plasticity during learning
 is studied through perceptron.
 A perceptron is a single neuron model which linearly classifies the input
 patterns into binary classes.
 In our case, the input patterns are random patterns each associated to
 a randomly selected binary output.
 The perceptron takes the input patterns and calculates the output which
 is matched with the desired output.
 The synaptic weights are updated when there is a mismatch between the actual
 and desired output according to the perceptron learning rule 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:1"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f=\begin{cases}
1 & if\sum_{i=0}^{N}w_{i}*x_{i}>0\\
0 & otherwise
\end{cases}\label{eq:1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Here 
\begin_inset Formula $\left.w_{i}\right.$
\end_inset

 is the weight of synapse 
\begin_inset Formula $\left.i\right.$
\end_inset

, 
\begin_inset Formula $\left.x_{i}\right.$
\end_inset

 is the 
\begin_inset Formula $\left.i^{th}\right.$
\end_inset

 input pattern, and 
\begin_inset Formula $\left.N\right.$
\end_inset

 is the number of synapses.
 
\begin_inset Formula $x_{0}$
\end_inset

 and 
\begin_inset Formula $w_{0}$
\end_inset

 are the bias values which are set to 1 and 0 respectively.
 The weights are updated for each pattern until all the patterns are correctly
 classified 
\begin_inset CommandInset citation
LatexCommand cite
key "10.1007/978-3-642-70911-1_20"
literal "false"

\end_inset

.
 The learning time is measured as the time taken for the perceptron to learn
 all the input patterns over multiple iterations, which is measured in epochs.
\end_layout

\begin_layout Standard
The amount of metabolic energy required to modify a synapse during the learning
 process is a parsimonious model 
\begin_inset CommandInset citation
LatexCommand cite
key "Li714055"
literal "false"

\end_inset

.
 In this model, the metabolic energy for every modification of synaptic
 weight is proportional to the amount of change of the weights.
 The total metabolic cost M to train a perceptron is the sum of the changes
 in the synaptic weights,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
M=\mathop{\sum_{i=1}^{N}}\sum_{t=1}^{T}|w_{i}(t)-w_{i}(t-1)|^{\alpha}\label{eq:2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\left.N\right.$
\end_inset

 is the number of synapses, 
\begin_inset Formula $w_{i}$
\end_inset

 is the weight of synapse 
\begin_inset Formula $\left.i\right.,$
\end_inset

 and 
\begin_inset Formula $\left.T\right.$
\end_inset

 is the total number of time-steps required for the perceptron to learn
 the inputs.
 The exponent 
\begin_inset Formula $\left.\alpha\right.$
\end_inset

 is set to 1.
\end_layout

\begin_layout Subsection*
Forgetting in a perceptron
\end_layout

\begin_layout Standard
Firstly, we tried passive forgetting in the perceptron which is implemented
 as a slow exponential decay of the synaptic weights.
 With the decay in place, we first tried to check the capacity of the perceptron
 which can be defined as the maximum number of patterns that the perceptron
 could successfully train.
 It is clear from the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2"
plural "false"
caps "false"
noprefix "false"

\end_inset

 that the energy required by the perceptron is directly proportional to
 the number of input patterns to be classified.
 The critical capacity of a perceptron with 
\begin_inset Formula $\left.N\right.$
\end_inset

 synapses and 
\begin_inset Formula $\left.P\right.$
\end_inset

 patterns is when 
\begin_inset Formula $\left.P=2N\right.$
\end_inset

.
 However, when there is a decay on the synaptic weights, the capacity of
 the perceptron will be less than the critical value.
 This is because the decay on the synaptic weights during the training process
 would forget (or unlearn) some of the previously learned patterns.
 In order to test this, we trained the perceptron for different values of
 synapses 
\begin_inset Formula $\left.N\right.$
\end_inset

 and calculated the capacity as a function of decay rate (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Maximum-capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 
\end_layout

\begin_layout Standard
For the given range of decay rates, the maximum capacity of the perceptron
 displays a linear relationship to the decay value.
 As the decay value increases, the maximum capacity of the perceptron decreases
 aiding to the decay of synaptic weights during the training period.
 A similar relationship can be seen in the energy expended by the perceptron
 for training the maximum number of patterns, where the total energy decreases
 with increase in the decay value.
 The relationship holds good for different values of 
\begin_inset Formula $\left.N\right.$
\end_inset

.
 This is because at zero decay on a log scale would yield the maximum capacity
 
\begin_inset Formula $\left.P=2N\right.$
\end_inset

 and for higher decay 
\begin_inset Formula $\left.P\right.$
\end_inset

 is always greater than 0.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/patterns.png
	scale 50

\end_inset


\begin_inset Graphics
	filename figures/energy.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
Maximum capacity and energy consumed: 
\series default
(left) The maximum capacity of the perceptron as a function of decay rate.
 This is calculated for different values of 
\begin_inset Formula $\left.N\right.$
\end_inset

.
 The dotted lines shows the actual curve and the solid line shows a straight
 line fit to the curve.
 (right) The energy required to train the maximum capacity of the perceptron
 as a function of decay rate.
 Each data point in both the graphs is an average of 50 runs.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Maximum-capacity"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
For the fitted lines in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Maximum-capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we calculated the slope of the lines and found a linear relationship between
 the slope and the number of synapses, 
\begin_inset Formula $\left.N\right.$
\end_inset

 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:slope"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Note that for 
\begin_inset Formula $\left.N=250\right.$
\end_inset

, a straight line is fit for the decay range 
\begin_inset Formula $\left.4*e^{-6}\right.$
\end_inset

 to 
\begin_inset Formula $\left.e^{-4}\right.$
\end_inset

.
 The coefficients of the fitted lines for different 
\begin_inset Formula $\left.N\right.$
\end_inset

 are shown in the table 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Coefficients"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "40line%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/slope.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
A plot of slope of the fitted lines in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Maximum-capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

 vs the number of synapses N.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:slope"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hspace{}
\length 2cm
\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "40line%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Slope
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Intercept
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1500
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-309.41380411
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-2346.92473118
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-210.26772697
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1458.99595753
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
500
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-210.26772697
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-524.04731183
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
250
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-44.34603812
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-150.74210526
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Coefficients - slope and intercept values of fitted lines for different
 N 
\begin_inset CommandInset label
LatexCommand label
name "fig:Coefficients"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
However, we see this linear relationship between patterns and decay rates
 for the medium range of decay values.
 This does not hold true when we considered low and high range of decay
 values, the curve tends to flatten for those values, resembling a sigmoid
 curve.
 We tested this for N=250 and the plot looks as shown in the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:sigmoid"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/patterns_250.png
	scale 60
	rotateOrigin center

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Maximum number of patterns trained by a perceptron for N=250 and for a wide
 range of decay values.
 Each data point is an average of 50 runs and the error bar is the standard
 deviation of the 50 runs.
\begin_inset CommandInset label
LatexCommand label
name "fig:sigmoid"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Accuracy of a perceptron
\end_layout

\begin_layout Standard
The accuracy of a perceptron is defined as the number of input patterns
 that are correctly classified out of all the input patterns presented.
 A perceptron with a passive decay on its weights may not always learn all
 the patterns during the training process.
 Depending on the value of the decay rate, the training process may reach
 a point where there is no improvement in the accuracy of the perceptron.
 So, it becomes imperative to quit the training early instead of trying
 to train the perceptron over multiple epochs with no improvement in its
 accuracy.
 In order to achieve this, we devised a method to monitor the accuracy of
 the perceptron over the training process and quit it when the accuracy
 no longer improves.
 In this method, we calculate the accuracy of the perceptron in each epoch
 and quit the training when the mean accuracy of the last 
\begin_inset Formula $\left.(n+1)\right.$
\end_inset

 to 
\begin_inset Formula $\left.2n\right.$
\end_inset

 epochs exceeds mean accuracy of the last 
\begin_inset Formula $\left.n\right.$
\end_inset

epochs.
 We calculated the optimal value of 
\begin_inset Formula $\left.n\right.$
\end_inset

, the window size, by training the perceptron with different values for
 
\begin_inset Formula $\left.n\right.$
\end_inset

 and analysing the value of accuracy with which the perceptron quits the
 training (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:window_size"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 We decided on the value of 
\begin_inset Formula $\left.n=25\right.$
\end_inset

 as the accuracy starts to plateau around that value.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/window_sizes.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Accuracy with which the perceptron quits for different 
\begin_inset Formula $\left.n\right.$
\end_inset

 where 
\begin_inset Formula $\left.n\right.$
\end_inset

 represents the window size.
 The value of decay was fixed to 
\begin_inset Formula $\left.1e^{-6}\right.$
\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "fig:window_size"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "references/perceptron,references/energy-efficient-synaptic-plasticity,references/overcoming-catastrophic-forgetting-in-neural-networks,references/catastrophic-forgetting,references/stability-plasticity,references/continual-learning-review,references/biology_of_forgetting,references/a-simple-weight-decay-can-improve-generalization,references/chaotic_nn"
options "apalike"

\end_inset


\end_layout

\end_body
\end_document
