#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\usepackage[font=scriptsize]{caption}
\captionsetup[figure]{font=scriptsize}
\hypersetup{linkcolor=blue, citecolor=blue}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{}\chead{}\rhead{}\lfoot{}\cfoot{\small\thepage}\rfoot{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\setlength{\footskip}{30pt}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "palatino" "EB Garamond"
\font_sans "helvet" "Arial"
\font_typewriter "courier" "Courier"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing onehalf
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 2cm
\headsep 2cm
\footskip 2cm
\secnumdepth -2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style british
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Does forgetting save the brain's metabolic energy for future learning?
\end_layout

\begin_layout Author
Pooja Nagendra Babu (20216094)
\begin_inset Newline newline
\end_inset

MSc, Computational Neuroscience, Cognition and AI
\begin_inset Newline newline
\end_inset

University of Nottingham
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList table

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{page}{1}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Learning in the brain is a process by which the brain encodes information
 of the surroundings in the form of synaptic connections.
 These connections, over time, strengthen to form memories, help us to adapt
 to the surroundings, and makes us who we are.
 The strengthening of connections, also known as synaptic plasticity, is
 the biological process by which specific patterns of neural activity change
 the strength and efficacy of synaptic connections.
 Synaptic plasticity determines how effectively neurons communicate with
 each other.
 It is the fundamental mechanism for learning and memory, and facilitates
 brain development and recovery from brain lesions.
 Many scientists have made important contributions to the understanding
 of synaptic plasticity, that dates back to the Spanish neuroanatomist Santiago
 Ramon y Cajal in the 1890s who suggested that the capacity of the brain
 could be augmented by increasing the number of connections 
\begin_inset CommandInset citation
LatexCommand cite
key "cajal1893nuevo"
literal "false"

\end_inset

.
 The properties of synaptic transmission rose to prominence during the twentieth
 century when Donald Hebb postulated that synapses change as a consequence
 of simultaneous firing form the neural basis of learning and memory 
\begin_inset CommandInset citation
LatexCommand cite
key "hebb-organization-of-behavior-1949"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
One of the phenomena underlying synaptic plasticity is long-term potentiation
 (LTP), which is the persistent strengthening of synapses based on the neural
 activity.
 This kind of synaptic plasticity can be transient, that lasts from milliseconds
 to a few minutes, and result in short-term synaptic facilitation and depression.
 This leads to early-phase LTP and forms short-term memories 
\begin_inset CommandInset citation
LatexCommand cite
key "doi:10.1146/annurev.physiol.64.092501.114547"
literal "false"

\end_inset

.
 On the other hand, the synaptic strength can last for days or weeks, leading
 to late-phase LTP.
 This phase is characterised by gene transcription and protein synthesis
 in the postsynaptic neuron.
 Experimental evidence supports this kind of plasticity in the dentate gyrus
 of the rabbit hippocampus 
\begin_inset CommandInset citation
LatexCommand cite
key "doi:10.1098/rstb.2002.1226"
literal "false"

\end_inset

.
 The phenomenon of late-phase LTP is known to be the neural basis of forming
 long-term memories and synaptic consolidation 
\begin_inset CommandInset citation
LatexCommand cite
key "Clopath"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
As memories start to accumulate in the brain over time, it may be beneficial
 for the brain to have a mechanism to eliminate memories that are obsolete.
 For example, retaining outdated memories might impede judgement and make
 it difficult to adjust to changing environment.
 Preserving strong and disabling memories associated with brain disorders
 like post-traumatic stress disorder (PTSD) can also be debilitating.
 This phenomenon of getting rid of unwanted memories, called forgetting,
 is considered as the flip side of learning.
 Forgetting can occur due to the failed retrieval of an intact engram through
 the biological degradation of molecular and cellular memory traces or when
 a fraction of engram cells become disconnected from the engram cell circuit
 
\begin_inset CommandInset citation
LatexCommand cite
key "DAVIS2017490"
literal "false"

\end_inset

.
 This form of forgetting is gradual and referred to as natural forgetting
 
\begin_inset CommandInset citation
LatexCommand cite
key "article"
literal "false"

\end_inset

.
 Forgetting can also be artificially induced with interventions by reversing
 learning-induced changes in the synaptic strength by manipulating a protein
 kinase C (PKC) isoform, PKM-
\begin_inset Formula $\zeta$
\end_inset

, that plays a key role in maintaining LTP and memory 
\begin_inset CommandInset citation
LatexCommand cite
key "articlePKM"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
Although forgetting appears to signify as a failure of the brain to recall
 memories, it is considered essential in processing incoming information.
 In a study involving Drosophila, forgetting is shown as an adaptive feature
 of the memory system to adjust to the changing environment by removing
 unrewarded memories 
\begin_inset CommandInset citation
LatexCommand cite
key "10.1371/journal.pcbi.1003640"
literal "false"

\end_inset

.
 A computational study by 
\begin_inset CommandInset citation
LatexCommand cite
key "article"
literal "false"

\end_inset

 shows that forgetting offers advantages for memory-guided decision making
 in environments that change and are noisy.
 They propose that forgetting enhances behavioural flexibility by eliminating
 outdated information and it helps to generalise by preventing overfitting
 memories to instances from the past that may not be helpful in predicting
 the future.
\end_layout

\begin_layout Standard
Learning and memory, specifically long-term memory, formed through synaptic
 plasticity is an energy intensive process 
\begin_inset CommandInset citation
LatexCommand cite
key "Mery1148"
literal "false"

\end_inset

.
 Studies on Drosophila flies show that they increase their glucose intake
 during late stages of long-term memory formation, especially in the neurons
 of the mushroom body which is the fly's main memory centre 
\begin_inset CommandInset citation
LatexCommand cite
key "Placais"
literal "false"

\end_inset

.
 Another study on Drosophila showed that under food shortage, the fly's
 brain disables the formation of energy intensive long-term memories as
 a strategy for survival 
\begin_inset CommandInset citation
LatexCommand cite
key "Placais440"
literal "false"

\end_inset

.
 They postulate that the shutdown of LTP upon starvation may correspond
 to a mechanism of conservation between energy homeostasis and the ability
 to form long-term memories.
\end_layout

\begin_layout Standard
A more recent computational study by 
\begin_inset CommandInset citation
LatexCommand cite
key "Li714055"
literal "false"

\end_inset

 shows that the more memories a network has, higher the metabolic cost to
 conduct future learning.
 Motivated by this, I investigate if forgetting some memories in the brain
 could conserve energy to learn new patterns in the future.
 I analyse this with the help of a single neuron model, also called a perceptron.
 I present input patterns to the perceptron and measure the energy consumption
 while I implement forgetting.
 I focus on two types of forgetting — passive forgetting and catastrophic
 forgetting — and examine the energy expended to learn new input patterns.
 I show that a trained perceptron with catastrophic forgetting saves more
 energy to learn new patterns when compared to a trained perceptron with
 passive forgetting.
\end_layout

\begin_layout Subsection
Types of forgetting
\end_layout

\begin_layout Standard
Traditionally, forgetting in the brain is regarded as a slow and natural
 decay of memories over time due to the general instability of biological
 mechanisms.
 This form of forgetting is referred to as 
\emph on
passive forgetting
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "DAVIS2017490"
literal "false"

\end_inset

.
 Passive forgetting may occur due to the loss of context cues over time
 or interference due to other similar memories.
 This kind of forgetting could be translated to decay in weights over time
 in the context of neural networks.
 This kind of decay can improve generalisation by suppressing irrelevant
 components of the weight vector and can suppress some effects of static
 noise on the targets 
\begin_inset CommandInset citation
LatexCommand cite
key "NIPS1991_563"
literal "false"

\end_inset

.
 The results from 
\begin_inset CommandInset citation
LatexCommand cite
key "LI2012412"
literal "false"

\end_inset

 establish that a Hebbian Learning rule with passive forgetting in a chaotic
 neural network (CNN) acts as a fuzzy-like pattern classifier that performs
 better than the ordinary CNN.
\end_layout

\begin_layout Standard
Learning can also happen continually where the networks learn by accommodating
 knowledge over time by learning a sequence of tasks.
 This process called continual learning is the ability to learn consecutive
 tasks without forgetting how to perform the previously learned tasks 
\begin_inset CommandInset citation
LatexCommand cite
key "Kirkpatrick3521"
literal "false"

\end_inset

.
 This poses a problem for the networks to learn a sequence of tasks where
 learning new information may degrade the performance of previous learned
 tasks due to information loss.
 This property of the networks to forget the old information when learning
 the new information is called 
\emph on
catastrophic forgetting
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "MCCLOSKEY1989109"
literal "false"

\end_inset

.
 It is considered as a form of 
\emph on
active forgetting 
\emph default
which in neuroscience is considered as a method where the brain has mechanisms
 to remove memories that become unused 
\begin_inset CommandInset citation
LatexCommand cite
key "DAVIS2017490"
literal "false"

\end_inset

.
 This can lead to a trade-off between the extent to which the system can
 become plastic in order to learn new information and remain stable in order
 to not catastrophically forget the old acquired knowledge, often referred
 to as the stability-plasticity dilemma 
\begin_inset CommandInset citation
LatexCommand cite
key "ABRAHAM200573"
literal "false"

\end_inset

.
 In contrast, humans and other animals can learn in a continual fashion
 in a lifelong manner.
 Experimental evidence suggests that in order to prevent catastrophic forgetting
, there are specialised systems in the hippocampus that allows for learning
 new information which, over time, will be transferred to the neocortical
 system for long-term storage 
\begin_inset CommandInset citation
LatexCommand cite
key "PARISI201954"
literal "false"

\end_inset

.
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Subsection
Learning in a perceptron
\end_layout

\begin_layout Standard
The metabolic energy expended due to synaptic plasticity during learning
 is studied in a perceptron.
 A perceptron is a single neuron model which linearly classifies the input
 patterns into binary classes.
 A simple perceptron, with its binary input and output, is used for modelling
 the operation of the cerebellar cortex 
\begin_inset CommandInset citation
LatexCommand cite
key "Brunel"
literal "false"

\end_inset

.
 In our case, the input patterns are random patterns each associated to
 a randomly selected binary output.
 The perceptron takes the input patterns and calculates the output which
 is matched with the desired output.
 The synaptic weights are updated when there is a mismatch between the desired
 and actual output 
\begin_inset Formula $\left.f\right.$
\end_inset

 according to the perceptron learning rule 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:1"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f=\begin{cases}
1 & if\sum_{i=0}^{N}w_{i}*x_{i}>0\\
0 & otherwise
\end{cases}\label{eq:1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Here 
\begin_inset Formula $\left.w_{i}\right.$
\end_inset

 is the weight of synapse 
\begin_inset Formula $\left.i\right.$
\end_inset

, 
\begin_inset Formula $\left.x_{i}\right.$
\end_inset

 is the 
\begin_inset Formula $\left.i^{th}\right.$
\end_inset

 input pattern, and 
\begin_inset Formula $\left.N\right.$
\end_inset

 is the number of synapses.
 
\begin_inset Formula $x_{0}$
\end_inset

 and 
\begin_inset Formula $w_{0}$
\end_inset

 are the bias values which are set to 1 and 0 respectively.
 The weights are updated for each pattern as per equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2"
plural "false"
caps "false"
noprefix "false"

\end_inset

 until all the patterns are correctly classified 
\begin_inset CommandInset citation
LatexCommand cite
key "10.1007/978-3-642-70911-1_20"
literal "false"

\end_inset

.
 
\begin_inset Formula 
\begin{equation}
w_{i}(t+1)=w_{i}(t)+\eta(d_{j}-f_{j}(t))x_{j,i}\label{eq:2}
\end{equation}

\end_inset

where 
\begin_inset Formula $\left.\eta\right.$
\end_inset

 is the learning rate which is set to 1, 
\begin_inset Formula $\left.d_{j}\right.$
\end_inset

 is the desired output.
 The learning time is measured as the time taken for the perceptron to learn
 all the input patterns over multiple iterations, which is measured in epochs.
\end_layout

\begin_layout Standard
The amount of metabolic energy required to modify a synapse during the learning
 process is a parsimonious model 
\begin_inset CommandInset citation
LatexCommand cite
key "Li714055"
literal "false"

\end_inset

.
 In this model, the metabolic energy for every modification of synaptic
 weight is proportional to the amount of change of the weights.
 The total metabolic cost M to train a perceptron is the sum of the changes
 in the synaptic weights,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
M=\mathop{\sum_{i=1}^{N}}\sum_{t=1}^{T}|w_{i}(t)-w_{i}(t-1)|^{\alpha}\label{eq:3}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $w_{i}$
\end_inset

 is the weight of synapse 
\begin_inset Formula $\left.i\right.,$
\end_inset

 and 
\begin_inset Formula $\left.T\right.$
\end_inset

 is the total number of time-steps required for the perceptron to learn
 the inputs.
 The exponent 
\begin_inset Formula $\left.\alpha\right.$
\end_inset

 is set to 1.
\end_layout

\begin_layout Subsection
How forgetting affects memory capacity
\end_layout

\begin_layout Standard
Firstly, I implement passive forgetting in the perceptron which is implemented
 as a slow exponential decay of the synaptic weights which occurs at every
 time-step.
 With the decay in place, I first try to check the capacity of the perceptron
 which can be defined as the maximum number of patterns that the perceptron
 could successfully train.
 It is clear from the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:3"
plural "false"
caps "false"
noprefix "false"

\end_inset

 that the energy required by the perceptron is linearly proportional to
 the number of input patterns to be classified as more patterns require
 longer training time.
 Without decay, the critical capacity of a perceptron with 
\begin_inset Formula $\left.N\right.$
\end_inset

 synapses and 
\begin_inset Formula $\left.P\right.$
\end_inset

 patterns is when 
\begin_inset Formula $\left.P=2N\right.$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Mitchison"
literal "false"

\end_inset

.
 However, when there is a decay on the synaptic weights, the maximum capacity
 of the perceptron will be less than the critical value.
 The decay on the synaptic weights during the training process would forget
 (or unlearn) some of the previously learned patterns resulting in the maximum
 capacity being 
\begin_inset Formula $P<2N$
\end_inset

.
 In order to test this, I train the perceptron for different values of synapses
 
\begin_inset Formula $\left.N\right.$
\end_inset

 and calculated the capacity as a function of decay rate (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Maximum-capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Standard
For the given range of decay rates, the maximum capacity of the perceptron
 displays a linear relationship with the decay value.
 As the decay value increases, the maximum capacity of the perceptron decreases
 aiding to the rapid decay of synaptic weights during the training period.
 A similar relationship can be seen in the energy expended by the perceptron
 for training the maximum number of patterns (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Energy-max-capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

 (left)).
 The total energy decreases with increase in the decay value.
 The larger the decay value, fewer the weight updates and lesser the energy
 required to train all the patterns presented to the perceptron.
 The relationship holds good for different values of 
\begin_inset Formula $\left.N\right.$
\end_inset

.
 The plot showing energy consumed per pattern is interesting (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Energy-max-capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

 (right)).
 The curves represents the plot of a convex-like function.
 For 
\begin_inset Formula $\left.N=1500\right.$
\end_inset

 and 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

, energy per pattern is high for decay 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

 as the capacity of the perceptron is high and thus requires more energy
 to train all the patterns.
 The value of energy per pattern decreases as the decay value approaches
 
\begin_inset Formula $\left.10^{-5}\right.$
\end_inset

 because the capacity decreases.
 However, as the decay reaches the value 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

, energy per pattern starts to increase.
 The high value of decay rate overpowers in this range and the perceptron
 updates the weights more often even though the capacity is low and consumes
 more energy.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center

\emph on
\begin_inset Graphics
	filename figures/patterns.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Maximum memory capacity of perceptron
\end_layout

\end_inset

(left) The maximum capacity of the perceptron as a function of decay rate.
 This is calculated for different values of 
\begin_inset Formula $\left.N\right.$
\end_inset

.
 The dotted lines shows the actual curve and the solid line shows a straight
 line fit to the curve.
 Each data point in both the graphs is an average of 50 runs.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Maximum-capacity"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/energy.png
	scale 55

\end_inset


\begin_inset Graphics
	filename figures/energy_per_pattern.png
	scale 55

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Energy consumed for maximum memory capacity
\end_layout

\end_inset

 (left) The energy required to train the maximum capacity of the perceptron
 as a function of decay rate.
 (right) Energy required to train per pattern when the perceptron reaches
 its maximum capacity.
 Each data point in both the graphs is an average of 50 runs.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Energy-max-capacity"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
For the fitted lines in the right graph of Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Maximum-capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

, I calculate the slope of the lines and find a linear relationship between
 the slope and the number of synapses, 
\begin_inset Formula $\left.N\right.$
\end_inset

 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:slope"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Note that for 
\begin_inset Formula $\left.N=250\right.$
\end_inset

, a straight line is fit for the decay range of 
\begin_inset Formula $\left.4.10^{-5}\right.$
\end_inset

 to 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

 only, while for other N, they are fitted from 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

 to 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

.
 The coefficients of the fitted lines for different 
\begin_inset Formula $\left.N\right.$
\end_inset

 are shown in the table 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Coefficients"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "40line%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/slope.png
	scale 55

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Slope vs #synapses
\end_layout

\end_inset

 A plot of slope of the fitted lines in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Maximum-capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

 vs the number of synapses N.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:slope"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hspace{}
\length 3cm
\end_inset


\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "40line%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
captionsetup{type=table}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
N
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
Slope
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
Intercept
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
1500
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
-309.41380411
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
-2346.92473118
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
-210.26772697
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
-1458.99595753
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
500
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
-210.26772697
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
-524.04731183
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
250
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
-44.34603812
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
-150.74210526
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Coefficients - slope and intercept
\end_layout

\end_inset

Coefficients - slope and intercept values of fitted lines for different
 N 
\begin_inset CommandInset label
LatexCommand label
name "fig:Coefficients"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Even though we see this linear relationship between patterns and decay rates
 for the medium range of decay values, this does not hold true when I consider
 lower and higher decay rates.
 The curve depicting maximum capacity tends to flatten for these values,
 resembling a sigmoid-like curve.
 This is because at zero decay on a log scale would yield the maximum capacity
 
\begin_inset Formula $\left.P=2N\right.$
\end_inset

 and for higher decay 
\begin_inset Formula $\left.P\right.$
\end_inset

 is always greater than 0.
 I tested this for 
\begin_inset Formula $\left.N=250\right.$
\end_inset

 and is as shown in the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:sigmoid"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/patterns_250.png
	scale 55
	rotateOrigin center

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Maximum capacity of a perceptron for N=250
\end_layout

\end_inset

 Maximum number of patterns trained by a perceptron for N=250 and for a
 wide range of decay values.
 Each data point is an average of 50 runs and the error bar is the standard
 deviation of the 50 runs.
\begin_inset CommandInset label
LatexCommand label
name "fig:sigmoid"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Forgetting affects accuracy
\end_layout

\begin_layout Standard
The performance of a perceptron is measured based on its ability to learn
 the input patterns and appropriately classify them.
 This measure, also called the accuracy of a perceptron, is defined as the
 number of input patterns that are correctly classified out of all the input
 patterns presented.
 A perceptron with a passive decay on its weights may not always learn all
 the patterns during the training process.
 Depending on the value of the decay rate, the training process may reach
 a point where there is no improvement in the accuracy of the perceptron.
 So, it becomes imperative to quit training at the right moment instead
 of trying to train the perceptron over multiple epochs with no improvement
 in its accuracy and waste unnecessary metabolic energy.
 
\end_layout

\begin_layout Standard
In order to achieve this, I devise a method to monitor the accuracy of the
 perceptron over the training process and quit it when the accuracy no longer
 improves.
 In this method, I calculate the accuracy of the perceptron in each epoch
 and quit the training when the mean accuracy of the last 
\begin_inset Formula $\left.(n+1)\right.$
\end_inset

 to 
\begin_inset Formula $\left.2n\right.$
\end_inset

 epochs exceeds or equals the mean accuracy of the last 
\begin_inset Formula $\left.n\right.$
\end_inset

 epochs.
 I calculate the optimal value of the window size 
\begin_inset Formula $\left.n\right.$
\end_inset

, by training the perceptron with different values of 
\begin_inset Formula $\left.n\right.$
\end_inset

 and analysing the value of accuracy with which the perceptron quits the
 training (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:window_size"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 The objective to calculate the optimal window size 
\begin_inset Formula $\left.n\right.$
\end_inset

 is, due to statistical fluctuation, if 
\begin_inset Formula $\left.n\right.$
\end_inset

 is too small, the perceptron can still improve its accuracy if it is allowed
 to train for a longer time.
 On the other hand, if 
\begin_inset Formula $\left.n\right.$
\end_inset

 is too large, the perceptron may over-train and over-estimate the metabolic
 energy cost.
 Hence, I decide on the optimal value as 
\begin_inset Formula $\left.n=25\right.$
\end_inset

 because the mean accuracy starts to plateau around that value.
 I performed this analysis for decay 
\begin_inset Formula $10^{-6}$
\end_inset

 and 
\begin_inset Formula $10^{-5}$
\end_inset

 and found that window of 
\begin_inset Formula $\left.n=25\right.$
\end_inset

 holds good for both values (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:window_size"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/window_sizes.png
	scale 55

\end_inset


\begin_inset Graphics
	filename figures/accuracy_1.png
	scale 55

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Window size for training the perceptron
\end_layout

\end_inset

 Accuracy with which the perceptron quits for different 
\begin_inset Formula $\left.n\right.$
\end_inset

 where 
\begin_inset Formula $\left.n\right.$
\end_inset

 represents the window size.
 The value of decay was fixed to 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

 in the left and 
\begin_inset Formula $10^{-5}$
\end_inset

 in the right figure 
\begin_inset CommandInset label
LatexCommand label
name "fig:window_size"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
I then test the behaviour of the perceptron by fixing the number of synapses
 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

 and the number of patterns to train as 
\begin_inset Formula $\left.1600\right.$
\end_inset

.
 I measure the accuracy, learning rate, and the energy consumed by the perceptro
n for a decay range of 
\begin_inset Formula $\left.10^{-8}\right.$
\end_inset

 to 
\begin_inset Formula $\left.10^{-2}\right.$
\end_inset

 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/accuracy.png
	scale 55

\end_inset


\begin_inset Graphics
	filename figures/epoch_accuracy.png
	scale 55

\end_inset


\begin_inset VSpace defskip
\end_inset


\begin_inset Graphics
	filename figures/energy_accuracy.png
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Accuracy, learning time, and energy consumed for a perceptron
\end_layout

\end_inset


\series default
 Accuracy, learning time, and energy consumed are measured for the perceptron
 with a decay in weights, resulting in passive forgetting.
 (Top left) Accuracy of the perceptron measured as the number of input patterns
 that are correctly classified among all the input patterns.
 (Top right) Epochs or the learning time of perceptron is the measure of
 the time (in epochs) required for the perceptron to learn input patterns
 until there is no further improvement in its accuracy.
 (Bottom) Energy expended during the training process.
 The dashed line indicates the energy value required to train the perceptron
 when there is no decay.
 The results indicate the values obtained by training the perceptron with
 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

 and 
\begin_inset Formula $\left.1600\right.$
\end_inset

 patterns over a decay range of 
\begin_inset Formula $\left.10^{-8}\right.$
\end_inset

 to 
\begin_inset Formula $\left.10^{-2}\right.$
\end_inset

.
 Each data point is the average of 
\begin_inset Formula $\left.50\right.$
\end_inset

 runs.
 The uncertainty shown for each data point is the standard deviation of
 
\begin_inset Formula $\left.50\right.$
\end_inset

 runs.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Performance"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
From figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can see that with the increase in decay rate of the weights, the accuracy
 of the perceptron decreases and plateaus for higher decay values.
 A similar pattern can be observed for epochs or the learning time of the
 perceptron, where the learning time is high for low decay rates as the
 perceptron has a larger capacity, whereas as the decay value increases,
 the perceptron capacity reduces and so does the learning time, until it
 plateaus for high decay values.
\end_layout

\begin_layout Standard
However, the trend of the curve showing the energy required for the perceptron
 to train the input patterns for a range of decay values is not straightforward
 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

 bottom).
 The energy expended by the perceptron to learn the patterns increases as
 the decay rate increases till the decay value reaches 
\begin_inset Formula $\left.10^{-5}\right.$
\end_inset

 after which the energy decreases till the decay value reaches 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

 after which the value of energy increases again.
 In order to gain more insight into this behaviour of energy consumed, I
 check how the weights of the perceptron are updated in each epoch during
 the process of training as the value of total energy expended is directly
 determined by the change of synaptic weights (Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:3"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Thus, I plot the number of updates to the synaptic weights versus each
 epoch in the training (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:epoch-updates"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 
\end_layout

\begin_layout Standard
Even though the number of updates to the weights is larger for the decay
 value of 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

, the perceptron has trained for a fewer epochs when compared to the decay
 value of 
\begin_inset Formula $\left.10^{-5}\right.$
\end_inset

 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:epoch-updates"
plural "false"
caps "false"
noprefix "false"

\end_inset

 left).
 So by the end of training, the perceptron has accumulated only a few synaptic
 weight updates in the former case and hence a dip in the energy curve for
 that decay value (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

 bottom).
 However, for the range of decay values in 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

 and 
\begin_inset Formula $\left.10^{-3}\right.$
\end_inset

, the energy expended by the perceptron increases again with the increase
 in decay.
 Clearly, the number of updates to the synaptic weights is more for decay
 value 
\begin_inset Formula $\left.10^{-3}\right.$
\end_inset

 when compared to the decay value of 
\begin_inset Formula $10^{-4}$
\end_inset

 even if there is not much of a difference in the number of epochs required
 for training (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:epoch-updates"
plural "false"
caps "false"
noprefix "false"

\end_inset

 right).
 Also, accuracy of the perceptron is higher for the latter case (Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

 top-left) accounting for fewer weight updates than the former.
 This explains why there in an increase in the energy curve in the decay
 range 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

 and 
\begin_inset Formula $\left.10^{-3}\right.$
\end_inset

.
\end_layout

\begin_layout Standard
To summarise, the energy required to train the perceptron increases as the
 decay increases because with the decay in weights the perceptron spends
 more time steps to learn the input patterns which leads to more updates
 in its weights thus leading to the increase in the total energy required
 for training.
 Although, for very low decay values, between 
\begin_inset Formula $\left.10^{-8}\right.$
\end_inset

 and 
\begin_inset Formula $\left.10^{-7}\right.$
\end_inset

, we can observe a small amount of energy being saved (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

 bottom) where the total number of updates to the weights is less than the
 scenario without decay.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/epoch_updates_1.png
	scale 55

\end_inset


\begin_inset Graphics
	filename figures/epoch_updates_2.png
	scale 55

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Synaptic weight updates during training
\end_layout

\end_inset

 The number of updates to the synaptic weights of the perceptron during
 the training process.
 (Left) The number of updates to weights for the decay values 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

, 
\begin_inset Formula $\left.10^{-5}\right.$
\end_inset

, and 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

.
 (Right) The number of updates to the synaptic weights over multiple runs
 for the decay values 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

 and 
\begin_inset Formula $\left.10^{-3}\right.$
\end_inset

.
 Each line on the plots represent a single run.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:epoch-updates"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Forgetting and energy conservation for future learning
\end_layout

\begin_layout Standard
In the previous section, we studied the accuracy of the perceptron while
 learning the input patterns with passive forgetting.
 We examined the energy conservation of the perceptron to learn new patterns
 while it was forgetting at the same time.
 However, biologically, memories are formed and stored initially and then
 forgotten with time if they are unwanted or unused.
 At the same time, the brain can learn and form new memories by consuming
 energy.
 Equating this to our model, I wanted to see in a perceptron that is already
 trained with some prior memories, if forgetting would be helpful to conserve
 energy, and use it to learn new patterns.
 In order to test this, I develop algorithms with passive forgetting and
 training schedules that lead to catastrophic forgetting and train the perceptro
n with prior memories to learn new patterns.
 I then compare the energy required to train the perceptron using each of
 the algorithms with a benchmark algorithm in which the perceptron is trained
 without forgetting.
\end_layout

\begin_layout Standard
Firstly, a perceptron with 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

 synapses is considered and trained for 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns without any decay on the synaptic weights.
 This forms a perceptron with prior memories.
 The perceptron is again trained with the first 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns along with an additional 
\begin_inset Formula $\left.100\right.$
\end_inset

 patterns.
 The energy expended in training these patterns would be the 
\emph on
benchmark
\emph default
 value that is used to compare with other algorithms.
 I develop three algorithms of training the perceptron with catastrophic
 forgetting and two algorithms with passive forgetting.
 
\end_layout

\begin_layout Standard
Catastrophic forgetting is implemented as intentional forgetting of some
 of the old patterns learnt by the perceptron along with interference by
 introducing some new patterns to train.
 Passive forgetting is implemented similar to the previous section where
 the synaptic weights decay away at a constant rate after each epoch of
 training, with an added task here of learning new patterns.
 All the algorithms are implemented on a perceptron with 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

 synaptic connections.
 The number of patterns used to train the perceptron with some prior memories,
 the number of patterns forgotten, and the number of new patterns to learn
 in each algorithm is listed in table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Number-of-patterns"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
The first algorithm is implemented by initially training the perceptron
 with 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns, then by training with a new set of 
\begin_inset Formula $\left.700\right.$
\end_inset

 randomly selected old patterns and 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns and measuring the energy required in training the new set.
 Note that here forgetting is implemented as intentional forgetting of some
 of the old patterns and interfering those by learning new patterns.
 The goal of this algorithm is to check the energy consumption with a new
 training set with total number of patterns in the new training set (
\begin_inset Formula $\left.800)\right.$
\end_inset

 less than the initial training set (
\begin_inset Formula $\left.1000\right.$
\end_inset

).
 This algorithm is called 
\emph on
catastrophic forgetting 1.
 
\end_layout

\begin_layout Standard
For the second algorithm, I initially train the perceptron with 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns and then completely forget the previous 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns and only train with 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns the second time and measure the energy consumed.
 The objective here is to measure the energy consumed with a significantly
 lower number of patterns in the new training set than the number used to
 train originally.
 This algorithm is named 
\emph on
catastrophic forgetting 2.
 
\end_layout

\begin_layout Standard
The third algorithm is implemented by initially training a perceptron with
 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns, then by training a new set of 
\begin_inset Formula $\left.900\right.$
\end_inset

 randomly selected old patterns and 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns, and measuring the energy expended in training the new set.
 The aim of this algorithm is to measure the energy consumption having a
 new training set 
\begin_inset Formula $\left.(900+100=1000)\right.$
\end_inset

 equal to the number of patterns in the original set 
\begin_inset Formula $\left.(1000)\right.$
\end_inset

.
 This algorithm is called 
\emph on
catastrophic forgetting 3.
\end_layout

\begin_layout Standard
We can observe from figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Forgetting-energy"
plural "false"
caps "false"
noprefix "false"

\end_inset

 that all the three catastrophic forgetting algorithms conserve some energy
 when compared with the benchmark value.
 The total number of patterns in the new training set for catastrophic forgettin
g 1 and catastrophic forgetting 2 algorithms is less than the number of
 patterns used for catastrophic forgetting 3 (Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Number-of-patterns"
plural "false"
caps "false"
noprefix "false"

\end_inset

 last column).
 Catastrophic forgetting 2 conserves the most energy because the new training
 set contains only the new patterns with all the originally trained patterns
 forgotten.
 The perceptron takes fewer epochs to quickly learn the new patterns which
 is far less than the patterns in the original set, thus spending less energy.
 
\end_layout

\begin_layout Standard
The total number of patterns in the final training set of catastrophic forgettin
g 1 algorithm is less than the number of patterns used in catastrophic forgettin
g 3.
 In the former case, a perceptron with prior memories learns the new set
 of patterns more quickly because more patterns are forgotten as opposed
 to the latter.
 This allows the perceptron to learn the new patterns with fewer updates
 to the weights without any interference to the old patterns, consequently
 expending less energy.
 Energy conservation with catastrophic forgetting 3 is noteworthy here because
 the number of patterns used to train originally is the same as the new
 training set with some forgotten patterns and new patterns.
 The perceptron clearly uses less energy for updating weights to learn new
 patterns while forgetting some old patterns in comparison to learning the
 same number of patterns from scratch.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align left

\size scriptsize
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="6">
<features tabularvalignment="middle">
<column alignment="center" valignment="middle" width="18text%">
<column alignment="center" valignment="middle" width="14text%">
<column alignment="center" valignment="middle" width="14text%">
<column alignment="center" valignment="middle" width="14text%">
<column alignment="center" valignment="middle" width="14text%">
<column alignment="center" valignment="middle" width="14text%">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
Algorithm
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
Initial training set
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
# patterns removed from the initial training set
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
# patterns retained from the initial training set
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
# new patterns
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
Total patterns: new training set
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
Benchmark
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1100
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
Catastrophic forgetting 1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
300
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
700
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
800
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
Catastrophic forgetting 2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
100
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
Catastrophic forgetting 3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
900
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
Passive forgetting 1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1100
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
Passive forgetting 2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
900
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\align left
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Patterns for forgetting and future learning
\end_layout

\end_inset

Number of patterns for training the perceptron with different algorithms.
 The number of synapses in the perceptron is set to 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "tab:Number-of-patterns"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/energy_bar.png
	scale 55

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Forgetting in perceptron and energy consumption for future learning
\end_layout

\end_inset


\series default
 (Left) Energy consumed for all the five different algorithms of learning
 in a perceptron with forgetting (catastrophic or passive).
 The perceptron has 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

 synapses and trained for 
\begin_inset Formula $\left.1000\right.$
\end_inset

 initial patterns.
 The decay value for passive forgetting is fixed to 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

.
 The energy data points in both the graphs is an average value of 
\begin_inset Formula $\left.50\right.$
\end_inset

 runs 
\begin_inset CommandInset label
LatexCommand label
name "fig:Forgetting-energy"

\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The last two algorithms are implemented with passive forgetting.
 In the first case, I train the perceptron initially with 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns with a decay on the synaptic weights, then train 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns along with the old patterns by retaining the decay on weights.
 This algorithm is called 
\emph on
passive forgetting 1.
 
\emph default
In the last algorithm, I first train the perceptron with 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns with a decay on the synaptic weights, then train with a new set
 of 
\begin_inset Formula $\left.900\right.$
\end_inset

 randomly selected old patterns along with 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns by retaining the decay on weights and measure the energy expended
 in training the new set.
 This algorithm is called 
\emph on
passive forgetting 2.
 
\emph default
The objective of the two algorithms is to examine if the gradual decay of
 memories conserve any energy to benefit future learning.
\end_layout

\begin_layout Standard
First the perceptron is trained with a fixed decay value of 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Forgetting-energy"
plural "false"
caps "false"
noprefix "false"

\end_inset

) for passive forgetting algorithms.
 The total number of patterns used for passive forgetting 2 is less than
 the patterns used for passive forgetting 1.
 Intuitively, it appears that the gradual decay of weights at every epoch
 of the training process should conserve more energy and make it easier
 for the perceptron for future learning.
 However, in reality, the constant decay of weights don't actually lead
 to forgetting specific memories.
 The weight decay could also correspond to the new memories which makes
 it harder for the perceptron to learn new memories.
 Therefore, the weights get frequently updated in the training process,
 inevitably increasing the energy consumption.
 This behaviour can be clearly observed in the energy consumption of passive
 forgetting 1 algorithm where the value exceeds the benchmark (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Forgetting-energy"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 On the other hand, passive forgetting 2 with a combination of decay in
 weights and intentional forgetting consumes less energy than the benchmark.
 This combination gives the perceptron a little leeway to learn the new
 patterns more effectively than if it had learnt all of them at once (benchmark).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/energy_forgetting_line.png
	scale 55

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Forgetting and energy consumption for future learning over a range of decay
 rates
\end_layout

\end_inset

The energy consumption measured over a range of decay values from 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

 to 
\begin_inset Formula $\left.5*10^{-6}\right.$
\end_inset

 for passive forgetting.
 The dashed lines represent the energy values for benchmark and three learning
 algorithms with catastrophic forgetting.
 The energy data points in both the graphs is an average value of 
\begin_inset Formula $\left.50\right.$
\end_inset

 runs.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:forgetting-energy-decay"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
I extended this analysis over a range of decay values (
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

 to 
\begin_inset Formula $\left.5*10^{-6}\right.$
\end_inset

) for passive forgetting algorithms such that the perceptron trained all
 the patterns with 
\begin_inset Formula $\left.100\%\right.$
\end_inset

 accuracy (figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:forgetting-energy-decay"
plural "false"
caps "false"
noprefix "false"

\end_inset

)).
 The energy values of catastrophic forgetting algorithms are not affected
 by this as they are not dependent on decay values.
 The passive forgetting 2 algorithm conserved energy and the amount of energy
 conserved decreased with increase in the decay value till 
\begin_inset Formula $\left.4.5*10^{-6}\right.$
\end_inset

 after which there was no gain in energy savings.
 The passive forgetting 1 algorithm did not conserve energy for any value
 of decay.
 Hence, the catastrophic forgetting algorithms perform well overall and
 conserve more energy as compared to passive forgetting algorithms.
\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
Synaptic plasticity is known to be the neural basis of learning in animals.
 The synapses strengthen during learning and leads to the formation of short-ter
m and long-term memories.
 Forgetting plays an integral part in the maintenance of these memories
 and helps the animals to adapt to varying environments.
 The formation of long-term memories through synaptic plasticity is an energy
 intensive process.
 It has been shown theoretically that networks with fewer memories use less
 energy on future learning, therefore I wondered if forgetting can help
 networks save energy on learning.
 With the help of a perceptron, I implemented two types of forgetting, namely
 passive and catastrophic forgetting.
 Firstly, I implemented passive forgetting as a constant decay on the synaptic
 weights of the perceptron and studied the maximum capacity of a perceptron.
 I showed that for different synaptic connections, the maximum capacity
 of the perceptron decreased with increase in the decay value.
 I also showed that the energy required to train the perceptron to its maximum
 capacity with passive forgetting decreased with increase in the decay value
 due to reduction in training time as the capacity decreases.
 Even though, biologically, not all the synaptic connections of a neuron
 contribute to memories, the result signifies the limitations on the capacity
 of forgetful networks to hold memories.
\end_layout

\begin_layout Standard
Secondly, I devised a method to measure the accuracy of the perceptron as
 a function of decay.
 I measured the accuracy, learning time, and the energy consumed by the
 perceptron and showed that the accuracy and learning time decreased with
 increase in decay till it flattened for high decay values.
 I demonstrated that passive forgetting caused a tiny amount of energy conservat
ion for very low decay values while the total energy expended increased
 as a function of decay.
\end_layout

\begin_layout Standard
Finally, in order to investigate if by forgetting old patterns the perceptron
 saved energy to learn new patterns, I used the catastrophic and passive
 forgetting mechanisms in the brain and implemented algorithms to train
 the perceptron.
 I demonstrated that catastrophic forgetting algorithms conserved significant
 amount of energy for future learning.
 Biologically, the intentional forgetting and interference mechanisms used
 in catastrophic forgetting algorithms is observed in an fMRI study of word-pict
ure associations that showed suppression of non-practiced memories by the
 way of forgetting by inhibition 
\begin_inset CommandInset citation
LatexCommand cite
key "Wimber"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
I also showed that passive forgetting algorithms do not perform very well
 with respect to conserving energy.
 Among them, passive forgetting 2 performed better than passive forgetting
 1.
 More realistically, learning in animals is not a onetime phenomenon.
 We normally learn something new multiple times by practice or recall spaced
 over multiple intervals.
 This could be mimicked in our model by extending the passive forgetting
 algorithm 2 to make the perceptron learn new patterns multiple times.
 For instance, the randomly selected 
\begin_inset Formula $\left.900\right.$
\end_inset

 old patterns in passive forgetting 2 could be trained in three cycles with
 
\begin_inset Formula $\left.300\right.$
\end_inset

 old patterns in each cycle along with 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns.
 This way, the perceptron would learn new patterns three times rather than
 only once.
 It would be helpful to investigate how much energy is conserved in this
 scenario.
\end_layout

\begin_layout Standard
The parsimonious model used to calculate energy considers only the metabolic
 cost of synaptic plasticity as it offers a significant contribution.
 In addition to that, when synaptic connections are changed, neural activities
 are altered resulting in spikes and synaptic transmission, which can lead
 to difference in energy consumption in the networks.
 While we could incorporate these to calculate the energy, the model becomes
 too complex to measure.
 The brain typically contains billions of neurons which are connected with
 trillions of connections.
 Although our analysis interpret the results for a single-neuron model,
 it would be beneficial to extend our study to a larger network of neurons
 with single or multiple layers as this would be more representative of
 the connections in the brain.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\end_layout

\begin_layout Plain Layout


\backslash
bibname{References}
\end_layout

\end_inset


\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "references/perceptron,references/energy-efficient-synaptic-plasticity,references/overcoming-catastrophic-forgetting-in-neural-networks,references/catastrophic-forgetting,references/stability-plasticity,references/continual-learning-review,references/biology_of_forgetting,references/a-simple-weight-decay-can-improve-generalization,references/chaotic_nn,references/cajal,references/hebb,references/short-term,references/ltp,references/brea,references/persistence_and_transience,references/a-cost-of-long-term-memory-in-drosophila,references/Primo_BibTeX_Export,references/food-shortage,references/PKM,references/synaptic-consolidation,references/per-capacity,references/suppression,references/purkinje"
options "apalike"

\end_inset


\end_layout

\end_body
\end_document
