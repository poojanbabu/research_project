#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\usepackage[font=scriptsize]{caption}
\captionsetup[figure]{font=scriptsize}
\hypersetup{linkcolor=blue, citecolor=blue}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{}\chead{}\rhead{}\lfoot{}\cfoot{\small\thepage}\rfoot{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\setlength{\footskip}{30pt}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "palatino" "EB Garamond"
\font_sans "helvet" "Arial"
\font_typewriter "courier" "Courier"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing onehalf
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 2cm
\headsep 2cm
\footskip 2cm
\secnumdepth -2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style british
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Does forgetting save the brain's metabolic energy for future learning?
\end_layout

\begin_layout Author
Pooja Nagendra Babu (20216094)
\begin_inset Newline newline
\end_inset

MSc, Computational Neuroscience, Cognition and AI
\begin_inset Newline newline
\end_inset

University of Nottingham
\end_layout

\begin_layout Section*
Abstract
\end_layout

\begin_layout Standard
The human brain only weighs 2% of the total body mass but is responsible
 for 20% of the body's resting metabolism.
 Therefore, it is critical that the brain is an energy-efficient organ.
 Theoretical evidence has suggested that the energy cost to form new memories
 rises steeply with the number of memories already stored in the system.
 Thus, the brain might have mechanisms to lower the memory load in order
 to decrease the energy consumption for future learning.
 That is, the brain forgets some memories to make future learning more energy-ef
ficient.
 In this project, I study the impact of forgetting on the amount of energy
 expended on future learning and network performance using a single-neuron
 model called the perceptron.
 By implementing forgetting as exponential decay of weights of the perceptron,
 I show that the maximum capacity of the perceptron decreases with an increase
 in decay rate.
 I devise a method to measure the accuracy of the perceptron as a function
 of decay value and show that accuracy and learning time decreases with
 increase in the decay rate, and energy increases with increase in decay
 rate.
 Lastly, I implement training schedules that would lead to catastrophic
 forgetting of some old memories to investigate if the perceptron saves
 energy for future learning when the network permits some of the initial
 learning to be forgotten.
 I show that such a strategy does conserve energy to learn new memories,
 whereas passive forgetting with decay on weights does not benefit in saving
 energy for future learning.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList table

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{page}{1}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Learning in the brain is a process by which the brain encodes information
 of the surroundings in the form of synaptic connections.
 These connections, over time, strengthen to form memories, help us to adapt
 to the surroundings, and makes us who we are.
 The strengthening of connections, also known as synaptic plasticity, is
 the biological process by which specific patterns of neural activity change
 the strength and efficacy of synaptic connections.
 It is the fundamental mechanism for learning and memory, and facilitates
 brain development and recovery from brain lesions.
 Many scientists have made important contributions to the understanding
 of synaptic plasticity, that dates back to the Spanish neuroanatomist Santiago
 Ramon y Cajal in the 1890s who suggested that the capacity of the brain
 could be augmented by increasing the number of connections 
\begin_inset CommandInset citation
LatexCommand cite
key "cajal1893nuevo"
literal "false"

\end_inset

.
 The properties of synaptic transmission rose to prominence during the twentieth
 century when Donald Hebb postulated that synapses change as a consequence
 of simultaneous firing, which forms the neural basis of learning and memory
 
\begin_inset CommandInset citation
LatexCommand cite
key "hebb-organization-of-behavior-1949"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
One of the phenomena underlying synaptic plasticity is long-term potentiation
 (LTP), which is the persistent strengthening of synapses based on the neural
 activity.
 There are two forms of LTP: early-phase LTP and late-phase LTP.
 If only the early-phase LTP occurs, which lasts for minutes to an hour,
 short-term memories are formed 
\begin_inset CommandInset citation
LatexCommand cite
key "doi:10.1146/annurev.physiol.64.092501.114547"
literal "false"

\end_inset

.
 On the other hand, late-phase LTP causes the synaptic strength to last
 for days or weeks.
 This phase is characterised by gene transcription and protein synthesis
 in the postsynaptic neuron.
 Experimental evidence supports this kind of plasticity in the dentate gyrus
 of the rabbit hippocampus 
\begin_inset CommandInset citation
LatexCommand cite
key "doi:10.1098/rstb.2002.1226"
literal "false"

\end_inset

.
 The phenomenon of late-phase LTP is known to be the neural basis of forming
 long-term memories and synaptic consolidation 
\begin_inset CommandInset citation
LatexCommand cite
key "Clopath"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
As memories start to accumulate in the brain over time, it may be beneficial
 for the brain to have a mechanism to eliminate memories that are obsolete.
 For example, retaining outdated memories might impede judgement and make
 it difficult to adjust to a changing environment.
 Preserving strong and disabling memories associated with brain disorders
 like post-traumatic stress disorder (PTSD) can also be debilitating.
 This phenomenon of getting rid of unwanted memories, called forgetting,
 is considered as the flip side of learning.
 Forgetting can occur due to the failed retrieval of an intact engram —
 molecular traces in a set of neurons that store the learned information
 
\begin_inset CommandInset citation
LatexCommand cite
key "SCHACTER1978721"
literal "false"

\end_inset

 — through the biological degradation of molecular and cellular memory traces
 or when a fraction of engram cells become disconnected from the engram
 cell circuit 
\begin_inset CommandInset citation
LatexCommand cite
key "DAVIS2017490"
literal "false"

\end_inset

.
 This form of forgetting is gradual and referred to as natural forgetting
 
\begin_inset CommandInset citation
LatexCommand cite
key "article"
literal "false"

\end_inset

.
 Forgetting can also be artificially induced with interventions by reversing
 learning-induced changes in the synaptic strength by manipulating a protein
 kinase C (PKC) isoform, PKM-
\begin_inset Formula $\zeta$
\end_inset

, that plays a key role in maintaining LTP and memory 
\begin_inset CommandInset citation
LatexCommand cite
key "articlePKM"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
Although forgetting appears to signify as a failure of the brain to recall
 memories, it is considered essential in processing incoming information.
 In a study involving Drosophila, forgetting is shown as an adaptive feature
 of the memory system to adjust to the changing environment by removing
 unrewarded memories 
\begin_inset CommandInset citation
LatexCommand cite
key "10.1371/journal.pcbi.1003640"
literal "false"

\end_inset

.
 A computational study by 
\begin_inset CommandInset citation
LatexCommand cite
key "article"
literal "false"

\end_inset

 shows that forgetting offers advantages for memory-guided decision making
 in environments that change and are noisy.
 They propose that forgetting enhances behavioural flexibility by eliminating
 outdated information and helps to generalise by preventing overfitting
 memories to instances from the past that may not be helpful in predicting
 the future.
\end_layout

\begin_layout Standard
Learning and memory, specifically long-term memory, formed through synaptic
 plasticity is an energy-intensive process 
\begin_inset CommandInset citation
LatexCommand cite
key "Mery1148"
literal "false"

\end_inset

.
 Studies on Drosophila flies show that they increase their glucose intake
 during late stages of long-term memory formation, especially in the neurons
 of the mushroom body which is the fly's main memory centre 
\begin_inset CommandInset citation
LatexCommand cite
key "Placais"
literal "false"

\end_inset

.
 Another study on Drosophila showed that under food shortage, the fly's
 brain disables the formation of energy-intensive long-term memories as
 a strategy for survival 
\begin_inset CommandInset citation
LatexCommand cite
key "Placais440"
literal "false"

\end_inset

.
 They postulate that the shutdown of late-phase LTP upon starvation may
 correspond to a mechanism of conservation between energy homeostasis and
 the ability to form long-term memories.
\end_layout

\begin_layout Standard
A more recent computational study by 
\begin_inset CommandInset citation
LatexCommand cite
key "Li714055"
literal "false"

\end_inset

 shows that the more memories a network has, the higher the metabolic cost
 to conduct future learning.
 Motivated by this, I investigate if forgetting some memories in the brain
 could conserve energy to learn new patterns in the future.
 I analyse this with the help of a single neuron model, also called a perceptron.
 I present input patterns to the perceptron and measure the energy consumption
 while I implement forgetting.
 I focus on two types of forgetting — passive forgetting and catastrophic
 forgetting — and examine the energy expended to learn new input patterns.
 I show that a trained perceptron with catastrophic forgetting saves more
 energy to learn new patterns when compared to a trained perceptron with
 passive forgetting.
\end_layout

\begin_layout Subsection
Types of forgetting
\end_layout

\begin_layout Standard
Traditionally, forgetting in the brain is regarded as a slow and natural
 decay of memories over time due to the general instability of biological
 mechanisms.
 This form of forgetting is referred to as 
\emph on
passive forgetting
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "DAVIS2017490"
literal "false"

\end_inset

.
 Passive forgetting may occur due to the loss of context cues over time
 or interference due to other similar memories.
 This kind of forgetting could be translated to decay in weights over time
 in the context of neural networks.
 This kind of decay can improve generalisation by suppressing irrelevant
 components of the weight vector and can suppress some effects of static
 noise on the targets 
\begin_inset CommandInset citation
LatexCommand cite
key "NIPS1991_563"
literal "false"

\end_inset

.
 The results from 
\begin_inset CommandInset citation
LatexCommand cite
key "LI2012412"
literal "false"

\end_inset

 establish that a Hebbian Learning rule with passive forgetting in a chaotic
 neural network (CNN) acts as a fuzzy-like pattern classifier that performs
 better than the ordinary CNN.
\end_layout

\begin_layout Standard
Learning can also happen continually where the networks learn by accommodating
 knowledge over time by learning a sequence of tasks.
 This process called continual learning is the ability to learn consecutive
 tasks without forgetting how to perform the previously learned tasks 
\begin_inset CommandInset citation
LatexCommand cite
key "Kirkpatrick3521"
literal "false"

\end_inset

.
 This poses a problem for the networks to learn a sequence of tasks where
 learning new information may degrade the performance of previously learned
 tasks due to information loss.
 This property of the networks to forget the old information when learning
 the new information is called 
\emph on
catastrophic forgetting
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "MCCLOSKEY1989109"
literal "false"

\end_inset

.
 It is considered as a form of 
\emph on
active forgetting 
\emph default
which in neuroscience is considered as a method where the brain has mechanisms
 to remove memories that become unused 
\begin_inset CommandInset citation
LatexCommand cite
key "DAVIS2017490"
literal "false"

\end_inset

.
 This can lead to a trade-off between the extent to which the system can
 become plastic in order to learn new information and remain stable in order
 to not catastrophically forget the old acquired knowledge, often referred
 to as the stability-plasticity dilemma 
\begin_inset CommandInset citation
LatexCommand cite
key "ABRAHAM200573"
literal "false"

\end_inset

.
 In contrast, humans and other animals can learn in a continual fashion
 in a lifelong manner.
 Experimental evidence suggests that in order to prevent catastrophic forgetting
, there are specialised systems in the hippocampus that allows for learning
 new information which, over time, will be transferred to the neocortical
 system for long-term storage 
\begin_inset CommandInset citation
LatexCommand cite
key "PARISI201954"
literal "false"

\end_inset

.
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Subsection
Learning in a perceptron
\end_layout

\begin_layout Standard
The metabolic energy expended due to synaptic plasticity during learning
 is studied in a perceptron.
 A perceptron is a single neuron model which linearly classifies the input
 patterns into binary classes.
 A simple perceptron, with its binary input and output, is used for modelling
 the operation of the cerebellar cortex 
\begin_inset CommandInset citation
LatexCommand cite
key "Brunel"
literal "false"

\end_inset

.
 In our case, the input patterns are random patterns each associated to
 a randomly selected binary output.
 The perceptron takes the input patterns and calculates the output which
 is matched with the desired output.
 The synaptic weights are updated when there is a mismatch between the desired
 and actual output 
\begin_inset Formula $\left.f\right.$
\end_inset

 according to the perceptron learning rule 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:1"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f=\begin{cases}
1 & if\sum_{i=0}^{N}w_{i}*x_{i}>0\\
0 & otherwise
\end{cases}\label{eq:1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Here 
\begin_inset Formula $\left.w_{i}\right.$
\end_inset

 is the weight of synapse 
\begin_inset Formula $\left.i\right.$
\end_inset

, 
\begin_inset Formula $\left.x_{i}\right.$
\end_inset

 is the 
\begin_inset Formula $\left.i^{th}\right.$
\end_inset

 input pattern, and 
\begin_inset Formula $\left.N\right.$
\end_inset

 is the number of synapses.
 
\begin_inset Formula $x_{0}$
\end_inset

 and 
\begin_inset Formula $w_{0}$
\end_inset

 are the bias values which are set to 1 and 0 respectively.
 The weights are updated for each pattern as per equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2"
plural "false"
caps "false"
noprefix "false"

\end_inset

 until all the patterns are correctly classified 
\begin_inset CommandInset citation
LatexCommand cite
key "10.1007/978-3-642-70911-1_20"
literal "false"

\end_inset

.
 
\begin_inset Formula 
\begin{equation}
w_{i}(t+1)=w_{i}(t)+\eta(d_{j}-f_{j}(t))x_{j,i}\label{eq:2}
\end{equation}

\end_inset

where 
\begin_inset Formula $\left.\eta\right.$
\end_inset

 is the learning rate which is set to 1, 
\begin_inset Formula $\left.d_{j}\right.$
\end_inset

 is the desired output.
 The learning time is measured as the time taken for the perceptron to learn
 all the input patterns over multiple iterations, which is measured in epochs.
\end_layout

\begin_layout Standard
The amount of metabolic energy required to modify a synapse during the learning
 process is a parsimonious model 
\begin_inset CommandInset citation
LatexCommand cite
key "Li714055"
literal "false"

\end_inset

.
 In this model, the metabolic energy for every modification of synaptic
 weight is proportional to the amount of change of the weights.
 The total metabolic cost M to train a perceptron is the sum of the changes
 in the synaptic weights,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
M=\mathop{\sum_{i=1}^{N}}\sum_{t=1}^{T}|w_{i}(t)-w_{i}(t-1)|^{\alpha}\label{eq:3}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $w_{i}$
\end_inset

 is the weight of synapse 
\begin_inset Formula $\left.i\right.$
\end_inset

, and 
\begin_inset Formula $\left.T\right.$
\end_inset

 is the total number of time-steps required for the perceptron to learn
 the inputs.
 The exponent 
\begin_inset Formula $\left.\alpha\right.$
\end_inset

 is set to 1.
\end_layout

\begin_layout Subsection
Forgetting affects memory capacity
\end_layout

\begin_layout Standard
Firstly, I implement passive forgetting in the perceptron which is implemented
 as a slow exponential decay of the synaptic weights which occurs at every
 time-step.
 With the decay in place, I try to check the capacity of the perceptron
 which can be defined as the maximum number of patterns that the perceptron
 could successfully train.
 It is clear from the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:3"
plural "false"
caps "false"
noprefix "false"

\end_inset

 that the energy required by the perceptron is proportional to the number
 of input patterns to be classified as more patterns require longer training
 time.
 Without decay, the critical capacity of a perceptron with 
\begin_inset Formula $\left.N\right.$
\end_inset

 synapses and 
\begin_inset Formula $\left.P\right.$
\end_inset

 patterns is when 
\begin_inset Formula $\left.P=2N\right.$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Mitchison"
literal "false"

\end_inset

.
 However, when there is decay on the synaptic weights, the maximum capacity
 of the perceptron will be less than the critical value.
 The decay on the synaptic weights during the training process would forget
 (or unlearn) some of the previously learned patterns resulting in the maximum
 capacity being 
\begin_inset Formula $P<2N$
\end_inset

.
 In order to test this, I train the perceptron for different values of synapses
 
\begin_inset Formula $\left.N\right.$
\end_inset

 and calculate the capacity as a function of the decay rate (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Maximum-capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Standard
For the given range of decay rates, the maximum capacity of the perceptron
 displays a linear relationship with the decay value.
 As the decay value increases, the maximum capacity of the perceptron decreases
 aiding to the rapid decay of synaptic weights during the training period.
 A similar relationship can be seen in the energy expended by the perceptron
 for training the maximum number of patterns (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Energy-max-capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

 (left)).
 The total energy decreases with the increase in the decay value.
 The larger the decay value, fewer the weight updates and lesser the energy
 required to train all the patterns presented to the perceptron due to decrease
 in the memory capacity.
 The relationship holds good for different values of 
\begin_inset Formula $\left.N\right.$
\end_inset

.
 
\end_layout

\begin_layout Standard
The plot showing the energy consumed per pattern is interesting (Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Energy-max-capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

 (right)).
 The curves represent the plot of a convex-like function.
 For 
\begin_inset Formula $\left.N=1500\right.$
\end_inset

 and 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

, energy per pattern is high for decay 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

 as the capacity of the perceptron is high and thus requires more energy
 to train all the patterns.
 The value of energy per pattern decreases as the decay value approaches
 
\begin_inset Formula $\left.10^{-5}\right.$
\end_inset

 because the capacity decreases.
 As the decay reaches the value 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

, energy per pattern starts to increase.
 In this range, the high value of decay rate overpowers and the perceptron
 updates the weights more often even though the capacity is low, resulting
 in more energy consumption.
 
\end_layout

\begin_layout Standard
However, for 
\begin_inset Formula $\left.N=500\right.$
\end_inset

 and 
\begin_inset Formula $\left.N=250\right.$
\end_inset

, the lowest points of the curves appears to be different, near or beyond
 the decay value 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

.
 This can be explained by looking at the total energy curves for different
 
\begin_inset Formula $\left.N\right.$
\end_inset

 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Energy-max-capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

 (left)).
 The curves for higher 
\begin_inset Formula $\left.N\right.$
\end_inset

 decrease faster with the increase in decay value in contrast to lower 
\begin_inset Formula $\left.N\right.$
\end_inset

.
 For lower 
\begin_inset Formula $\left.N\right.$
\end_inset

, the value of decay is not high enough to have a significant contribution
 on weight updates and hence energy.
 Thus, the lowest points of energy per pattern for lower 
\begin_inset Formula $\left.N\right.$
\end_inset

 are beyond the decay value 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center

\emph on
\begin_inset Graphics
	filename figures/patterns.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Maximum memory capacity of perceptron
\end_layout

\end_inset

The maximum capacity of the perceptron as a function of decay rate.
 This is calculated for different values of 
\begin_inset Formula $\left.N\right.$
\end_inset

.
 The dotted lines shows the actual curve and the solid line shows a straight
 line fit to the curve.
 Each data point the graph is an average of 50 runs.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Maximum-capacity"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/energy.png
	scale 55

\end_inset


\begin_inset Graphics
	filename figures/energy_per_pattern.png
	scale 55

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Energy consumed for maximum memory capacity
\end_layout

\end_inset

 (left) The energy required to train the maximum capacity of the perceptron
 as a function of decay rate.
 (right) Energy required to train per pattern when the perceptron reaches
 its maximum capacity.
 Each data point in both the graphs is an average of 50 runs.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Energy-max-capacity"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
For the fitted lines in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Maximum-capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

, I calculate the slope of the lines and find a linear relationship between
 the slope and the number of synapses, 
\begin_inset Formula $\left.N\right.$
\end_inset

 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:slope"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Note that for 
\begin_inset Formula $\left.N=250\right.$
\end_inset

, a straight line is fit for the decay range of 
\begin_inset Formula $\left.4.10^{-5}\right.$
\end_inset

 to 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

 only, while for other N, they are fitted from 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

 to 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

.
 The coefficients of the fitted lines for different 
\begin_inset Formula $\left.N\right.$
\end_inset

 are shown in the table 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Coefficients"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "40line%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/slope.png
	scale 55

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Slope vs #synapses
\end_layout

\end_inset

 A plot of slope of the fitted lines in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Maximum-capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

 vs the number of synapses N.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:slope"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hspace{}
\length 3cm
\end_inset


\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "40line%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
captionsetup{type=table}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
N
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
Slope
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
Intercept
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
1500
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
-309.41380411
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
-2346.92473118
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
-210.26772697
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
-1458.99595753
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
500
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
-210.26772697
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
-524.04731183
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
250
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
-44.34603812
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
-150.74210526
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Coefficients - slope and intercept
\end_layout

\end_inset

Coefficients - slope and intercept values of fitted lines for different
 N 
\begin_inset CommandInset label
LatexCommand label
name "fig:Coefficients"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Even though we see this linear relationship between patterns and decay rates
 for the medium range of decay values, this does not hold true when I consider
 lower and higher decay rates.
 The curve depicting maximum capacity tends to flatten for these values,
 resembling a sigmoid-like curve.
 This is because at zero decay on a log scale would yield the maximum capacity
 
\begin_inset Formula $\left.P=2N\right.$
\end_inset

 and for higher decay 
\begin_inset Formula $\left.P\right.$
\end_inset

 is always greater than 0.
 I tested this for 
\begin_inset Formula $\left.N=250\right.$
\end_inset

 and is as shown in the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:sigmoid"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/patterns_250.png
	scale 55
	rotateOrigin center

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Maximum capacity of a perceptron for N=250
\end_layout

\end_inset

 Maximum number of patterns trained by a perceptron for N=250 and for a
 wide range of decay values.
 Each data point is an average of 50 runs and the error bar is the standard
 deviation of the 50 runs.
\begin_inset CommandInset label
LatexCommand label
name "fig:sigmoid"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Forgetting affects accuracy
\end_layout

\begin_layout Standard
The performance of a perceptron is measured based on its ability to learn
 the input patterns and appropriately classify them.
 This measure, also called the accuracy of a perceptron, is defined as the
 number of input patterns that are correctly classified out of all the input
 patterns presented.
 A perceptron with a passive decay on its weights may not always learn all
 the patterns during the training process.
 Depending on the value of the decay rate, the training process may reach
 a point where there is no improvement in the accuracy of the perceptron.
 So, it becomes imperative to quit training at the right moment instead
 of trying to train the perceptron over multiple epochs with no improvement
 in its accuracy and waste unnecessary metabolic energy.
 
\end_layout

\begin_layout Standard
In order to achieve this, I devise a method to monitor the accuracy of the
 perceptron over the training process and quit it when the accuracy no longer
 improves.
 In this method, I calculate the accuracy of the perceptron in each epoch
 and quit the training when the mean accuracy of the last 
\begin_inset Formula $\left.(n+1)\right.$
\end_inset

 to 
\begin_inset Formula $\left.2n\right.$
\end_inset

 epochs exceeds or equals the mean accuracy of the last 
\begin_inset Formula $\left.n\right.$
\end_inset

 epochs.
 I calculate the optimal value of the window size 
\begin_inset Formula $\left.n\right.$
\end_inset

, by training the perceptron with different values of 
\begin_inset Formula $\left.n\right.$
\end_inset

 and analysing the value of accuracy at which the perceptron quits the training
 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:window_size"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 The objective to calculate the optimal window size 
\begin_inset Formula $\left.n\right.$
\end_inset

 is, if 
\begin_inset Formula $\left.n\right.$
\end_inset

 is too small, the perceptron can quit training prematurely due to statistical
 fluctuations.
 On the other hand, if 
\begin_inset Formula $\left.n\right.$
\end_inset

 is too large, the perceptron may over-train and over-estimate the metabolic
 energy cost.
 Hence, I decide on the optimal value as 
\begin_inset Formula $\left.n=25\right.$
\end_inset

 because the mean accuracy starts to plateau around that value.
 I performed this analysis for decay 
\begin_inset Formula $10^{-6}$
\end_inset

 and 
\begin_inset Formula $10^{-5}$
\end_inset

 and found that window of 
\begin_inset Formula $\left.n=25\right.$
\end_inset

 holds good for both values (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:window_size"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/window_sizes.png
	scale 55

\end_inset


\begin_inset Graphics
	filename figures/accuracy_1.png
	scale 55

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Window size for training the perceptron
\end_layout

\end_inset

 Accuracy with which the perceptron quits for different 
\begin_inset Formula $\left.n\right.$
\end_inset

 where 
\begin_inset Formula $\left.n\right.$
\end_inset

 represents the window size.
 The value of decay was fixed to 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

 in the left and 
\begin_inset Formula $10^{-5}$
\end_inset

 in the right figure 
\begin_inset CommandInset label
LatexCommand label
name "fig:window_size"

\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
I then test the behaviour of the perceptron by fixing the number of synapses
 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

 and the number of patterns to train as 
\begin_inset Formula $\left.1600\right.$
\end_inset

.
 I measure the accuracy, learning time, and the energy consumed by the perceptro
n for a decay range of 
\begin_inset Formula $\left.10^{-8}\right.$
\end_inset

 to 
\begin_inset Formula $\left.10^{-2}\right.$
\end_inset

 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/accuracy.png
	scale 55

\end_inset


\begin_inset Graphics
	filename figures/epoch_accuracy.png
	scale 55

\end_inset


\begin_inset VSpace defskip
\end_inset


\begin_inset Graphics
	filename figures/energy_accuracy.png
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Accuracy, learning time, and energy consumed for a perceptron
\end_layout

\end_inset


\series default
 Accuracy, learning time, and energy consumed are measured for the perceptron
 with a decay in weights, resulting in passive forgetting.
 (Top left) Accuracy of the perceptron measured as the number of input patterns
 that are correctly classified among all the input patterns.
 (Top right) Epochs or the learning time of perceptron is the measure of
 the time (in epochs) required for the perceptron to learn input patterns
 until there is no further improvement in its accuracy.
 (Bottom) Energy expended during the training process.
 The dashed line indicates the energy value required to train the perceptron
 when there is no decay.
 The results indicate the values obtained by training the perceptron with
 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

 and 
\begin_inset Formula $\left.1600\right.$
\end_inset

 patterns over a decay range of 
\begin_inset Formula $\left.10^{-8}\right.$
\end_inset

 to 
\begin_inset Formula $\left.10^{-2}\right.$
\end_inset

.
 Each data point is the average of 
\begin_inset Formula $\left.50\right.$
\end_inset

 runs.
 The uncertainty shown for each data point is the standard deviation of
 
\begin_inset Formula $\left.50\right.$
\end_inset

 runs.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Performance"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
From figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can see that with the increase in decay rate of the weights, the accuracy
 of the perceptron decreases and plateaus for higher decay values.
 A similar pattern can be observed for epochs or the learning time of the
 perceptron, where the learning time is high for low decay rates as the
 perceptron has a larger capacity, whereas as the decay value increases,
 the perceptron capacity reduces and so does the learning time until it
 plateaus for high decay values.
\end_layout

\begin_layout Standard
However, the trend of the curve showing the energy required for the perceptron
 to train the input patterns for a range of decay values is not straightforward
 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

 bottom).
 The energy expended by the perceptron to learn the patterns increases as
 the decay rate increases till the decay value reaches 
\begin_inset Formula $\left.10^{-5}\right.$
\end_inset

 after which the energy decreases till the decay value reaches 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

 after which the value of energy increases again.
 In order to gain more insight into this behaviour of energy consumed, I
 check how the weights of the perceptron are updated in each epoch during
 the process of training as the value of total energy expended is directly
 determined by the change of synaptic weights (Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:3"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Thus, I plot the number of updates to the synaptic weights versus each
 epoch in the training (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:epoch-updates"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 
\end_layout

\begin_layout Standard
The number of updates to the weights for the decay value of 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

 is fewer than the updates for decay 
\begin_inset Formula $10^{-5}$
\end_inset

 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:epoch-updates"
plural "false"
caps "false"
noprefix "false"

\end_inset

 left).
 Hence the total energy consumed for 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

 is less than for 
\begin_inset Formula $\left.10^{-5}\right.$
\end_inset

.
 The number of updates to the weights increase as the decay rate approaches
 
\begin_inset Formula $\left.10^{-5}\right.$
\end_inset

 which is reflected as increase in the energy expended.
\end_layout

\begin_layout Standard
Even though the number of updates to the weights is larger for the decay
 value of 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

, the perceptron has trained for a fewer epochs when compared to the decay
 value of 
\begin_inset Formula $\left.10^{-5}\right.$
\end_inset

 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:epoch-updates"
plural "false"
caps "false"
noprefix "false"

\end_inset

 left).
 So by the end of training, the perceptron has accumulated only a few synaptic
 weight updates in the former case and hence a dip in the energy curve for
 that decay value (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

 bottom).
\end_layout

\begin_layout Standard
However, for the range of decay values in 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

 and 
\begin_inset Formula $\left.10^{-3}\right.$
\end_inset

, the energy expended by the perceptron increases again with the increase
 in decay.
 Clearly, the number of updates to the synaptic weights is more for decay
 value 
\begin_inset Formula $\left.10^{-3}\right.$
\end_inset

 when compared to the decay value of 
\begin_inset Formula $10^{-4}$
\end_inset

 even if there is not much of a difference in the number of epochs required
 for training (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:epoch-updates"
plural "false"
caps "false"
noprefix "false"

\end_inset

 right).
 Also, accuracy of the perceptron is higher for the latter case (Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

 top-left) accounting for fewer weight updates than the former.
 This explains why there in an increase in the energy curve in the decay
 range 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

 and 
\begin_inset Formula $\left.10^{-3}\right.$
\end_inset

.
\end_layout

\begin_layout Standard
To summarise, the energy required to train the perceptron increases as the
 decay increases because with the decay in weights the perceptron spends
 more time steps to learn the input patterns which leads to more updates
 in its weights thus leading to the increase in the total energy required
 for training.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/epoch_updates_1.png
	scale 55

\end_inset


\begin_inset Graphics
	filename figures/epoch_updates_2.png
	scale 55

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Synaptic weight updates during training
\end_layout

\end_inset

 The number of updates to the synaptic weights of the perceptron during
 the training process.
 (Left) The number of updates to weights for the decay values 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

, 
\begin_inset Formula $\left.10^{-5}\right.$
\end_inset

, and 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

.
 (Right) The number of updates to the synaptic weights over multiple runs
 for the decay values 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

 and 
\begin_inset Formula $\left.10^{-3}\right.$
\end_inset

.
 Each line on the plots represent a single run.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:epoch-updates"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Forgetting and energy conservation for future learning
\end_layout

\begin_layout Standard
In the previous section, we studied the accuracy of the perceptron while
 learning the input patterns with passive forgetting.
 We examined the energy consumption of the perceptron to learn new patterns
 while it was forgetting at the same time.
 However, biologically, memories are formed and stored initially and with
 time obsolete memories are forgotten.
 This has been shown to be beneficial in animal's survival 
\begin_inset CommandInset citation
LatexCommand cite
key "10.1371/journal.pcbi.1003640"
literal "false"

\end_inset

.
 At the same time, the brain consumes energy when it learns and forms new
 memories.
 A computational study by 
\begin_inset CommandInset citation
LatexCommand cite
key "Li714055"
literal "false"

\end_inset

 showed that the network spends more energy on future learning if it has
 a larger memory load.
 Drawing inspiration from this, I wanted to investigate if by allowing the
 network to decrease the memory load through forgetting, the network saved
 energy for future learning.
 
\end_layout

\begin_layout Standard
In order to test this, I develop two kinds of algorithms: one with passive
 forgetting, and another with training schedules that lead to catastrophic
 forgetting.
 I train the perceptron with a set of patterns which are considered as 'prior
 memories', followed by learning of new patterns.
 I then compare the energy required to train the perceptron using each of
 the algorithms with a benchmark algorithm in which the perceptron is trained
 without forgetting.
\end_layout

\begin_layout Standard
To give more details about the training procedure, a perceptron with 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

 synapses is considered and trained with an 
\emph on
initial training set
\emph default
 that contains 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns without any decay on the synaptic weights.
 This forms a perceptron with prior memories.
 For the 
\emph on
benchmark 
\emph default
algorithm, the perceptron is then trained with a 
\emph on
new training set 
\emph default
that consists of the first 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns along with an additional 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns.
 The energy expended in training these patterns would be the 
\emph on
benchmark
\emph default
 value that is used to compare with other algorithms.
 I develop three algorithms of training the perceptron that allows for catastrop
hic forgetting of some of the prior memories, one algorithm with passive
 forgetting, and another with the combination of the two.
 
\end_layout

\begin_layout Standard
Catastrophic forgetting is implemented as intentional forgetting of some
 of the old patterns from the initial training set learnt by the perceptron,
 with interference by introducing some new patterns to train while excluding
 those old patterns from the new training set.
 The algorithms with passive forgetting are implemented similar to the previous
 section where the synaptic weights decay away at a constant exponential
 rate after each epoch of training, with an added task here of learning
 new patterns.
 All the algorithms are implemented on a perceptron with 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

 synaptic connections.
 The number of patterns used to train the perceptron with some prior memories,
 the number of patterns allowed to be forgotten, the number of prior memories
 retained, and the number of patterns in the new training set for each algorithm
 is listed in table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Number-of-patterns"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
The first algorithm is implemented by initially training the perceptron
 with 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns, then by training with a new set of 
\begin_inset Formula $\left.700\right.$
\end_inset

 randomly selected old patterns and 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns and measuring the energy required in training the new set.
 Note that here forgetting is implemented as intentional forgetting of some
 of the old patterns and interfering those by learning new patterns.
 The goal of this algorithm is to check the energy consumption with a new
 training set with total number of patterns in the new training set (
\begin_inset Formula $\left.800)\right.$
\end_inset

 less than the initial training set (
\begin_inset Formula $\left.1000\right.$
\end_inset

).
 This algorithm is called 
\emph on
catastrophic forgetting 1.
 
\end_layout

\begin_layout Standard
For the second algorithm, I initially train the perceptron with 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns and then allow the previous 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns to be interfered by only training with 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns the second time and measure the energy consumed.
 The objective here is to measure the energy consumed with a significantly
 lower number of patterns in the new training set than the number used to
 train originally.
 This algorithm is named 
\emph on
catastrophic forgetting 2.
 
\end_layout

\begin_layout Standard
The third algorithm is implemented by initially training a perceptron with
 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns, then by training a new set of 
\begin_inset Formula $\left.900\right.$
\end_inset

 randomly selected old patterns and 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns, and measuring the energy expended in training the new set.
 The aim of this algorithm is to measure the energy consumption having a
 new training set 
\begin_inset Formula $\left.(900+100=1000)\right.$
\end_inset

 equal to the number of patterns in the original set 
\begin_inset Formula $\left.(1000)\right.$
\end_inset

.
 This algorithm is called 
\emph on
catastrophic forgetting 3.
\end_layout

\begin_layout Standard
We can observe from figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Forgetting-energy"
plural "false"
caps "false"
noprefix "false"

\end_inset

 that all the three catastrophic forgetting algorithms conserve some energy
 when compared with the benchmark value.
 The total number of patterns in the new training set for catastrophic forgettin
g 1 and catastrophic forgetting 2 algorithms is less than the number of
 patterns used for catastrophic forgetting 3 (Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Number-of-patterns"
plural "false"
caps "false"
noprefix "false"

\end_inset

 last column).
 Catastrophic forgetting 2 conserves the most energy because the new training
 set contains only the new patterns with all the originally trained patterns
 free to be forgotten.
 The perceptron takes fewer epochs to quickly learn the new patterns which
 are far less than the patterns in the original set, thus spending less
 energy.
 
\end_layout

\begin_layout Standard
The total number of patterns in the final training set of catastrophic forgettin
g 1 algorithm is less than the number of patterns used in catastrophic forgettin
g 3.
 In the former case, a perceptron with prior memories learns the new set
 of patterns more quickly because more old patterns are no longer required
 to be memorised as opposed to the latter.
 This allows the perceptron to learn the new patterns with fewer updates
 to the weights, consequently expending less energy.
 Energy conservation with catastrophic forgetting 3 is noteworthy here because
 the number of patterns used to train originally (
\begin_inset Formula $\left.1000\right.$
\end_inset

) is the same as the new training set, which is made up of 
\begin_inset Formula $\left.900\right.$
\end_inset

 old patterns and 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns.
 The perceptron clearly uses less energy for updating weights to learn 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns while retaining 
\begin_inset Formula $\left.900\right.$
\end_inset

 old patterns in comparison to learning the same number of patterns from
 scratch.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align left

\size scriptsize
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="6">
<features tabularvalignment="middle">
<column alignment="center" valignment="middle" width="18text%">
<column alignment="center" valignment="middle" width="14text%">
<column alignment="center" valignment="middle" width="14text%">
<column alignment="center" valignment="middle" width="14text%">
<column alignment="center" valignment="middle" width="14text%">
<column alignment="center" valignment="middle" width="14text%">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
Algorithm
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
Initial training set
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
# patterns removed from the initial training set
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
# patterns retained from the initial training set
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
# new patterns
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
Total patterns: new training set
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
Benchmark
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1100
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
Catastrophic forgetting 1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
300
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
700
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
800
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
Catastrophic forgetting 2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
100
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
Catastrophic forgetting 3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
900
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
Passive forgetting 1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1100
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
Passive forgetting 2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
900
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\size scriptsize
1000
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\align left
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Patterns for forgetting and future learning
\end_layout

\end_inset

Number of patterns for training the perceptron with different algorithms.
 The number of synapses in the perceptron is set to 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "tab:Number-of-patterns"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/energy_bar.png
	scale 55

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Forgetting in perceptron and energy consumption for future learning
\end_layout

\end_inset


\series default
 Energy consumed to learn a new training set for all the five different
 algorithms of learning in a perceptron with forgetting (catastrophic or
 passive).
 Here, the perceptron has 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

 synapses and trained for 
\begin_inset Formula $\left.1000\right.$
\end_inset

 initial patterns.
 Afterwards, the perceptron is trained with a new training set that contains
 a certain amount of initial patterns decided by individual algorithms and
 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns.
 The decay value for passive forgetting is fixed to 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

.
 The energy data points is an average value of 
\begin_inset Formula $\left.50\right.$
\end_inset

 runs 
\begin_inset CommandInset label
LatexCommand label
name "fig:Forgetting-energy"

\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The last two algorithms are implemented with passive forgetting.
 In the first case, I train the perceptron initially with 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns with decay on the synaptic weights, then train 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns along with the old patterns with the same decay rate on weights.
 This algorithm is called 
\emph on
passive forgetting 1.
 
\emph default
In the last algorithm, I first train the perceptron with 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns with decay on the synaptic weights, then train with a new set
 of 
\begin_inset Formula $\left.900\right.$
\end_inset

 randomly selected old patterns by allowing 
\begin_inset Formula $\left.100\right.$
\end_inset

 old patterns to be forgotten catastrophically, along with 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns with the same decay rate on weights and measure the energy
 expended in training the new set.
 This algorithm is called 
\emph on
passive forgetting 2.
 
\emph default
The objective of the two algorithms is to examine if the gradual decay of
 memories conserves any energy to benefit future learning.
\end_layout

\begin_layout Standard
First, the perceptron is trained with a fixed decay value of 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Forgetting-energy"
plural "false"
caps "false"
noprefix "false"

\end_inset

) for passive forgetting algorithms.
 The total number of patterns used for passive forgetting 2 is less than
 the patterns used for passive forgetting 1.
 
\end_layout

\begin_layout Standard
In order to clearly understand the energy consumption in these two scenarios,
 we must revisit how the perceptron learns.
 Learning in a perceptron is understood as a search for a weight vector
 in the space of synaptic weights that classifies all the patterns.
 The search of synaptic weights follows a random walk until a solution for
 the weight vector is found, and the metabolic cost of learning is proportional
 to the length of the random walk 
\begin_inset CommandInset citation
LatexCommand cite
key "Li714055"
literal "false"

\end_inset

.
 Thus, intuitively, it appears that the gradual decay of weights at every
 epoch of the training process in passive forgetting would help in turning
 the search space of the weight vector of the perceptron to the right direction
 much faster such that all the patterns are classified correctly, as a result,
 conserving energy in the process.
\end_layout

\begin_layout Standard
However, in reality, the constant decay of weights don't actually lead to
 forgetting specific memories.
 The weight decay could also correspond to the new memories which makes
 it harder for the perceptron to learn new memories.
 Therefore, the weights get frequently updated in the training process,
 increasing the length of the random walk, and inevitably increasing the
 energy consumption.
 This behaviour can be clearly observed in the energy consumption of passive
 forgetting 1 algorithm where the value exceeds the benchmark (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Forgetting-energy"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 On the other hand, passive forgetting 2 with a combination of decay in
 weights and intentional forgetting consumes less energy than the benchmark.
 This combination gives the perceptron a little leeway to learn the new
 patterns more effectively than if it had learnt all of them at once (benchmark).
\end_layout

\begin_layout Standard
It is now interesting to compare the results of catastrophic forgetting
 3 and passive forgetting 2 algorithms as they both have the same total
 number of patterns (
\begin_inset Formula $\left.900+100\right.$
\end_inset

) in the new training set, with the only difference to the decay of weights
 in the latter.
 As discussed above passive forgetting 2 performs better than the benchmark
 because the perceptron was allowed to forget 
\begin_inset Formula $\left.100\right.$
\end_inset

 patterns catastrophically.
 However, it does not perform better than catastrophic forgetting 3 because
 the additional decay of synaptic weights in passive forgetting 2 further
 contributes to consuming more energy.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/energy_forgetting_line.png
	scale 55

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Forgetting and energy consumption for future learning over a range of decay
 rates
\end_layout

\end_inset

The energy consumption measured over a range of decay values from 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

 to 
\begin_inset Formula $\left.5*10^{-6}\right.$
\end_inset

 for passive forgetting.
 The dashed lines represent the energy values for benchmark and three learning
 algorithms with catastrophic forgetting.
 The energy data points in this graph is an average value of 
\begin_inset Formula $\left.50\right.$
\end_inset

 runs.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:forgetting-energy-decay"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
I extend this analysis over a range of decay values (
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

 to 
\begin_inset Formula $\left.5*10^{-6}\right.$
\end_inset

) for passive forgetting algorithms such that the perceptron trained all
 the patterns with 
\begin_inset Formula $\left.100\%\right.$
\end_inset

 accuracy (figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:forgetting-energy-decay"
plural "false"
caps "false"
noprefix "false"

\end_inset

)).
 The energy values of catastrophic forgetting algorithms are not affected
 by this as they are not dependent on the decay values.
 The passive forgetting 2 algorithm conserves energy compared to benchmark
 because the perceptron was allowed to forget some of the prior memories.
 The amount of energy conserved decreases with the increase in the decay
 value till 
\begin_inset Formula $\left.4.5*10^{-6}\right.$
\end_inset

 after which there is no gain in energy savings.
 This is because the weights decay faster and the perceptron has to spend
 more energy into updating the weights to learn new patterns.
 The passive forgetting 1 algorithm did not conserve energy for any value
 of decay showing that forgetting with only decay on weights is not beneficial
 for future learning.
 Hence, the catastrophic forgetting algorithms perform well overall and
 conserve more energy as compared to passive forgetting algorithms.
\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
Synaptic plasticity is known to be the neural basis of learning in animals.
 The synapses strengthen during learning and lead to the formation of short-term
 and long-term memories.
 Forgetting plays an integral part in the maintenance of these memories
 and helps the animals to adapt to varying environments.
 The formation of long-term memories through synaptic plasticity is an energy-in
tensive process.
 It has been shown theoretically that networks with fewer memories use less
 energy on future learning, therefore I wondered if forgetting can help
 networks save energy on learning.
 With the help of a perceptron, I induced algorithms with passive forgetting
 and training schedules that led to catastrophic forgetting.
 Firstly, I implemented passive forgetting as a constant decay on the synaptic
 weights of the perceptron and studied the maximum capacity of a perceptron.
 I showed that for a different number of synaptic connections, the maximum
 capacity of the perceptron decreased with the increase in the decay value.
 I also showed that the energy required to train the perceptron to its maximum
 capacity with passive forgetting decreased with the increase in the decay
 value due to a reduction in training time as the capacity decreased.
 Even though biologically, not all the synaptic connections of a neuron
 contribute to memories, the result signifies the limitations on the capacity
 of forgetful networks to hold memories.
\end_layout

\begin_layout Standard
Secondly, I devised a method to measure the accuracy of the perceptron as
 a function of the decay rate.
 I measured the accuracy, learning time, and the energy consumed by the
 perceptron and showed that the accuracy and learning time decreased with
 increase in decay till it flattened for high decay values, and the total
 energy consumed increased with increase in decay values.
\end_layout

\begin_layout Standard
Finally, in order to investigate if old patterns that are allowed to be
 forgotten in the perceptron saved energy to learn new patterns, I used
 the passive forgetting and interference mechanisms leading to catastrophic
 forgetting in the brain and implemented algorithms to train the perceptron.
 I demonstrated that catastrophic forgetting algorithms conserved a significant
 amount of energy for future learning.
 Biologically, the intentional forgetting and interference mechanisms used
 in catastrophic forgetting algorithms is observed in an fMRI study of word-pict
ure associations that showed suppression of non-practised memories by the
 way of forgetting by inhibition 
\begin_inset CommandInset citation
LatexCommand cite
key "Wimber"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
I showed that passive forgetting algorithms do not perform very well with
 respect to conserving energy because of the additional decay on weights.
 I also showed that even though passive forgetting 2 appears to conserve
 some amount of energy for future learning, it does so due to the prior
 memories being forgotten catastrophically and not due to decay on its weights.
 I attested this by comparing it with the energy consumed due to passive
 forgetting 1 which forgets only with decay on its weights.
 Overall, I concluded that algorithms with catastrophic forgetting performed
 better and conserved energy for future learning as opposed to passive forgettin
g algorithms.
\end_layout

\begin_layout Standard
More realistically, learning in animals is not a onetime phenomenon.
 We normally learn something new multiple times by practice or recall spaced
 over multiple intervals.
 This could be mimicked in our model by changing the passive forgetting
 algorithm 2 with more frequent exposure of new patterns to the perceptron.
 For instance, at each epoch, the randomly selected 
\begin_inset Formula $\left.900\right.$
\end_inset

 old patterns in passive forgetting 2 could be trained in three subsets
 with 
\begin_inset Formula $\left.300\right.$
\end_inset

 old patterns in each subset along with 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns.
 This way, the perceptron would learn new patterns three times rather than
 only once at each epoch.
 Perhaps, with this new schedule that trains new patters more often than
 the prior memories, passive forgetting could save energy.
 However, due to time constraints, I could not test this and would have
 to leave this investigation for future study.
\end_layout

\begin_layout Standard
In this study, energy is calculated by a parsimonious model with only the
 metabolic cost of synaptic plasticity as it offers a significant contribution.
 In reality, when synaptic connections are changed, neuronal properties
 are altered, which can lead to a difference in energy consumption in the
 networks.
 While we could incorporate these to calculate energy, the model becomes
 too complex to measure.
 
\end_layout

\begin_layout Standard
The brain typically contains billions of neurons which are connected with
 trillions of connections.
 Although our analysis interprets the results for a single-neuron model,
 it would be beneficial to extend our study to a larger network of neurons
 with single or multiple layers as this would be more representative of
 the connections in the brain.
\end_layout

\begin_layout Section
Acknowledgements
\end_layout

\begin_layout Standard
I would like to thank my supervisor Ho Ling Li for assisting me through
 this project.
 Her guidance helped me to understand the topic better; the discussions,
 her invaluable inputs and feedback throughout aided me to complete this
 research project.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\end_layout

\begin_layout Plain Layout


\backslash
bibname{References}
\end_layout

\end_inset


\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "references/perceptron,references/energy-efficient-synaptic-plasticity,references/overcoming-catastrophic-forgetting-in-neural-networks,references/catastrophic-forgetting,references/stability-plasticity,references/continual-learning-review,references/biology_of_forgetting,references/a-simple-weight-decay-can-improve-generalization,references/chaotic_nn,references/cajal,references/hebb,references/short-term,references/ltp,references/brea,references/persistence_and_transience,references/a-cost-of-long-term-memory-in-drosophila,references/Primo_BibTeX_Export,references/food-shortage,references/PKM,references/synaptic-consolidation,references/per-capacity,references/suppression,references/purkinje,references/engram"
options "apalike"

\end_inset


\end_layout

\end_body
\end_document
