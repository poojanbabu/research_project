#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\usepackage[font=scriptsize]{caption}
\captionsetup[figure]{font=scriptsize}
\hypersetup{linkcolor=blue, citecolor=blue}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "palatino" "EB Garamond"
\font_sans "helvet" "Arial"
\font_typewriter "courier" "Courier"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing onehalf
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 2cm
\headsep 2cm
\footskip 2cm
\secnumdepth -2
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style british
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Does forgetting save the brain's metabolic energy for future learning?
\end_layout

\begin_layout Author
Pooja Nagendra Babu (20216094)
\begin_inset Newline newline
\end_inset

MSc, Computational Neuroscience, Cognition and AI
\begin_inset Newline newline
\end_inset

University of Nottingham
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Learning in the brain is a process by which the brain encodes information
 of the surroundings in the form of synaptic connections.
 These connections, over time, strengthen to form memories, help us to adapt
 to the surroundings, and makes us who we are.
 The strengthening of connections, also known as synaptic plasticity, is
 the biological process by which specific patterns of neural activity changes
 the strength and efficacy of synaptic connections.
 Synaptic plasticity determines how effectively neurons communicate with
 each other, aids in learning and memory, brain development and recovery
 from brain lesions.
 Several scientists have made important contributions to the understanding
 of synaptic plasticity, that dates back to Ramon y Cajal in the 1890s who
 suggested that the capacity of the brain could be augmented by increasing
 the number of connections 
\begin_inset CommandInset citation
LatexCommand cite
key "cajal1893nuevo"
literal "false"

\end_inset

.
 The properties of synaptic transmission rose into prominence during the
 twentieth century when Donald Hebb postulated that synapses that change
 as a consequence of simultaneous firing form the neural basis of learning
 and memory 
\begin_inset CommandInset citation
LatexCommand cite
key "hebb-organization-of-behavior-1949"
literal "false"

\end_inset

.
 The synaptic plasticity can be transient, that lasts from milliseconds
 to a few minutes, can result in short-term synaptic facilitation and depression
 resulting in short-term memory 
\begin_inset CommandInset citation
LatexCommand cite
key "doi:10.1146/annurev.physiol.64.092501.114547"
literal "false"

\end_inset

.
 On the other hand, frequent neuronal activation can enhance synaptic transmissi
on for days or weeks, also known as long term potentiation (LTP).
 Experimental evidence supports this kind of plasticity in the dentate gyrus
 of the rabbit hippocampus 
\begin_inset CommandInset citation
LatexCommand cite
key "doi:10.1098/rstb.2002.1226"
literal "false"

\end_inset

.
 The phenomenon of LTP is known to be the neural basis for forming long-term
 memories.
\end_layout

\begin_layout Standard
As the memories start to accumulate in the brain over time, it becomes imperativ
e for the brain to have a mechanism to eliminate the memories that are obsolete.
 This phenomenon, called forgetting, is considered as the flip side of learning.
 Forgetting can occur due to the failed retrieval of an intact engram through
 the biological degradation of molecular and cellular memory traces or when
 a fraction of engram cells become disconnected from the engram cell circuit
 
\begin_inset CommandInset citation
LatexCommand cite
key "DAVIS2017490"
literal "false"

\end_inset

.
 This form of forgetting is gradual and referred to as natural forgetting
 
\begin_inset CommandInset citation
LatexCommand cite
key "article"
literal "false"

\end_inset

.
 Forgetting can also be artificially induced with interventions by reversing
 learning-induced changes in the synaptic strength by manipulating a protein
 kinase C (PKC) isoform, PKM-
\begin_inset Formula $\zeta$
\end_inset

, that plays a key role in maintaining LTP and memory 
\begin_inset CommandInset citation
LatexCommand cite
key "articlePKM"
literal "false"

\end_inset

.
 Although forgetting appears to signify as a failure of the brain to recall
 memories, it is considered essential in processing incoming information.
 In a study involving Drosophila, forgetting is shown as an adaptive feature
 of the memory system to adjust to the changing environment by reducing
 unrewarded memories 
\begin_inset CommandInset citation
LatexCommand cite
key "10.1371/journal.pcbi.1003640"
literal "false"

\end_inset

.
 A computational study by 
\begin_inset CommandInset citation
LatexCommand cite
key "article"
literal "false"

\end_inset

 shows that forgetting offers advantages for memory-guided decision making
 in environments that change and are noisy.
 They propose that forgetting enhances behavioural flexibility by eliminating
 outdated information and it helps to generalise by preventing overfitting
 memories to instances from the past that may not be helpful in predicting
 the future.
\end_layout

\begin_layout Standard
Learning and memory, specifically long-term memory, formed through synaptic
 plasticity is an energy intensive process 
\begin_inset CommandInset citation
LatexCommand cite
key "Mery1148"
literal "false"

\end_inset

.
 Studies on Drosophila flies show that they increase their glucose intake
 during early stages of long-term memory formation, especially in the neurons
 of the mushroom body which is the fly's main memory centre 
\begin_inset CommandInset citation
LatexCommand cite
key "Placais"
literal "false"

\end_inset

.
 Another study on Drosophila showed that under food shortage, the fly's
 brain disables the formation of energy intensive long-term memories as
 a strategy for survival 
\begin_inset CommandInset citation
LatexCommand cite
key "Placais440"
literal "false"

\end_inset

.
 They postulate that the shutdown of long-term memories upon starvation
 may correspond to a mechanism of conservation between energy homeostasis
 and the ability to form long-term memories.
 A more recent computational study by 
\begin_inset CommandInset citation
LatexCommand cite
key "Li714055"
literal "false"

\end_inset

 devise a energy-efficient synaptic plasticity algorithm which shows the
 benefit of transient forms of plasticity to explore the weight space and
 find suitable weights expending less energy.
 Motivated by these experimental studies, we investigate if forgetting some
 memories in the brain could conserve energy to learn new patterns in the
 future.
 We analyse this with the help of a single neuron model, also called a perceptro
n.
 We present input patterns to the perceptron and measure the energy consumption
 while we implement forgetting.
 We focus on two types of forgetting - passive forgetting and catastrophic
 forgetting - and examine the energy expended to learn new input patterns.
 We show that a trained perceptron with catastrophic forgetting saves more
 energy to learn new patterns when compared to a perceptron with passive
 forgetting.
\end_layout

\begin_layout Subsection
Types of forgetting
\end_layout

\begin_layout Standard
Traditionally, forgetting in the brain is regarded as a slow and natural
 decay of memories over time due to the general instability of biological
 mechanisms.
 This form of forgetting is referred to as 
\emph on
passive forgetting
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "DAVIS2017490"
literal "false"

\end_inset

.
 Passive forgetting may occur due to the loss of context cues over time
 or interference due to other similar memories.
 This kind of forgetting could be translated to decay in weights over time
 in the context of neural networks.
 This kind of decay can improve generalisation by suppressing irrelevant
 components of the weight vector and can suppress some effects of static
 noise on the targets 
\begin_inset CommandInset citation
LatexCommand cite
key "NIPS1991_563"
literal "false"

\end_inset

.
 The results from 
\begin_inset CommandInset citation
LatexCommand cite
key "LI2012412"
literal "false"

\end_inset

 establish that a Hebbian Learning rule with passive forgetting in a chaotic
 neural network (CNN) acts as a fuzzy-like pattern classifier that performs
 better than the ordinary CNN.
\end_layout

\begin_layout Standard
Learning can also happen continually where the networks learn by accommodating
 knowledge over time by learning a sequence of tasks.
 This process called continual learning is the ability to learn consecutive
 tasks without forgetting how to perform the previously learned tasks 
\begin_inset CommandInset citation
LatexCommand cite
key "Kirkpatrick3521"
literal "false"

\end_inset

.
 This poses a problem for the networks to learn a sequence of tasks where
 learning new information may degrade the performance due to the information
 loss of the previously learned tasks.
 This property of the networks to forget the old information when learning
 the new information is called 
\emph on
catastrophic forgetting
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "MCCLOSKEY1989109"
literal "false"

\end_inset

, which is considered as a form of 
\emph on
active forgetting
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "DAVIS2017490"
literal "false"

\end_inset

.
 This can lead to a trade-off between the extent to which the system can
 become plastic in order to learn new information and stable in order to
 not catastrophically forget the old acquired knowledge, often referred
 to as the stability-plasticity dilemma 
\begin_inset CommandInset citation
LatexCommand cite
key "ABRAHAM200573"
literal "false"

\end_inset

.
 In contrast, humans and other animals can learn in a continual fashion
 in a lifelong manner.
 Experimental evidence suggests that in order to prevent catastrophic forgetting
, there are specialised systems in the hippocampus that allows for learning
 new information which, over time, will be transferred to the neocortical
 system for long-term storage 
\begin_inset CommandInset citation
LatexCommand cite
key "PARISI201954"
literal "false"

\end_inset

.
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Subsection
Learning in a perceptron
\end_layout

\begin_layout Standard
The metabolic energy expended due to synaptic plasticity during learning
 is studied in a perceptron.
 A perceptron is a single neuron model which linearly classifies the input
 patterns into binary classes.
 In our case, the input patterns are random patterns each associated to
 a randomly selected binary output.
 The perceptron takes the input patterns and calculates the output which
 is matched with the desired output.
 The synaptic weights are updated when there is a mismatch between the actual
 and desired output according to the perceptron learning rule 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:1"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f=\begin{cases}
1 & if\sum_{i=0}^{N}w_{i}*x_{i}>0\\
0 & otherwise
\end{cases}\label{eq:1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Here 
\begin_inset Formula $\left.w_{i}\right.$
\end_inset

 is the weight of synapse 
\begin_inset Formula $\left.i\right.$
\end_inset

, 
\begin_inset Formula $\left.x_{i}\right.$
\end_inset

 is the 
\begin_inset Formula $\left.i^{th}\right.$
\end_inset

 input pattern, and 
\begin_inset Formula $\left.N\right.$
\end_inset

 is the number of synapses.
 
\begin_inset Formula $x_{0}$
\end_inset

 and 
\begin_inset Formula $w_{0}$
\end_inset

 are the bias values which are set to 1 and 0 respectively.
 The weights are updated for each pattern until all the patterns are correctly
 classified 
\begin_inset CommandInset citation
LatexCommand cite
key "10.1007/978-3-642-70911-1_20"
literal "false"

\end_inset

.
 The learning time is measured as the time taken for the perceptron to learn
 all the input patterns over multiple iterations, which is measured in epochs.
\end_layout

\begin_layout Standard
The amount of metabolic energy required to modify a synapse during the learning
 process is a parsimonious model 
\begin_inset CommandInset citation
LatexCommand cite
key "Li714055"
literal "false"

\end_inset

.
 In this model, the metabolic energy for every modification of synaptic
 weight is proportional to the amount of change of the weights.
 The total metabolic cost M to train a perceptron is the sum of the changes
 in the synaptic weights,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
M=\mathop{\sum_{i=1}^{N}}\sum_{t=1}^{T}|w_{i}(t)-w_{i}(t-1)|^{\alpha}\label{eq:2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\left.N\right.$
\end_inset

 is the number of synapses, 
\begin_inset Formula $w_{i}$
\end_inset

 is the weight of synapse 
\begin_inset Formula $\left.i\right.,$
\end_inset

 and 
\begin_inset Formula $\left.T\right.$
\end_inset

 is the total number of time-steps required for the perceptron to learn
 the inputs.
 The exponent 
\begin_inset Formula $\left.\alpha\right.$
\end_inset

 is set to 1.
\end_layout

\begin_layout Subsection
Forgetting in a perceptron
\end_layout

\begin_layout Standard
Firstly, we tried passive forgetting in the perceptron which is implemented
 as a slow exponential decay of the synaptic weights.
 With the decay in place, we first tried to check the capacity of the perceptron
 which can be defined as the maximum number of patterns that the perceptron
 could successfully train.
 It is clear from the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2"
plural "false"
caps "false"
noprefix "false"

\end_inset

 that the energy required by the perceptron is directly proportional to
 the number of input patterns to be classified.
 The critical capacity of a perceptron with 
\begin_inset Formula $\left.N\right.$
\end_inset

 synapses and 
\begin_inset Formula $\left.P\right.$
\end_inset

 patterns is when 
\begin_inset Formula $\left.P=2N\right.$
\end_inset

.
 However, when there is a decay on the synaptic weights, the maximum capacity
 of the perceptron will be less than the critical value.
 The decay on the synaptic weights during the training process would forget
 (or unlearn) some of the previously learned patterns resulting in the maximum
 capacity being 
\begin_inset Formula $P<2N$
\end_inset

.
 In order to test this, we trained the perceptron for different values of
 synapses 
\begin_inset Formula $\left.N\right.$
\end_inset

 and calculated the capacity as a function of decay rate (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Maximum-capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 The decay value is an exponential decay on weights and is applied on the
 weights at every time step of the training process.
\end_layout

\begin_layout Standard
For the given range of decay rates, the maximum capacity of the perceptron
 displays a linear relationship with the decay value.
 As the decay value increases, the maximum capacity of the perceptron decreases
 aiding to the rapid decay of synaptic weights during the training period.
 A similar relationship can be seen in the energy expended by the perceptron
 for training the maximum number of patterns, where the total energy decreases
 with increase in the decay value.
 The larger the decay value, fewer the weight updates and lesser the energy
 required to train all the patterns presented to the perceptron.
 The relationship holds good for different values of 
\begin_inset Formula $\left.N\right.$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center

\emph on
\begin_inset Graphics
	filename figures/patterns.png
	scale 50

\end_inset


\begin_inset Graphics
	filename figures/energy.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
Maximum capacity and energy consumed: 
\series default
(left) The maximum capacity of the perceptron as a function of decay rate.
 This is calculated for different values of 
\begin_inset Formula $\left.N\right.$
\end_inset

.
 The dotted lines shows the actual curve and the solid line shows a straight
 line fit to the curve.
 (right) The energy required to train the maximum capacity of the perceptron
 as a function of decay rate.
 Each data point in both the graphs is an average of 50 runs.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Maximum-capacity"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
For the fitted lines in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Maximum-capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we calculated the slope of the lines and found a linear relationship between
 the slope and the number of synapses, 
\begin_inset Formula $\left.N\right.$
\end_inset

 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:slope"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Note that for 
\begin_inset Formula $\left.N=250\right.$
\end_inset

, a straight line is fit for the decay range 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

 to 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

.
 The coefficients of the fitted lines for different 
\begin_inset Formula $\left.N\right.$
\end_inset

 are shown in the table 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Coefficients"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "40line%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figures/slope.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
A plot of slope of the fitted lines in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Maximum-capacity"
plural "false"
caps "false"
noprefix "false"

\end_inset

 vs the number of synapses N.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:slope"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hspace{}
\length 2cm
\end_inset


\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "40line%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
captionsetup{type=table}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Slope
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Intercept
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1500
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-309.41380411
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-2346.92473118
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-210.26772697
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1458.99595753
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
500
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-210.26772697
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-524.04731183
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
250
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-44.34603812
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-150.74210526
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Coefficients - slope and intercept values of fitted lines for different
 N 
\begin_inset CommandInset label
LatexCommand label
name "fig:Coefficients"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
However, we see this linear relationship between patterns and decay rates
 for the medium range of decay values.
 This does not hold true when we considered low and high decay rates.
 The curve depicting maximum capacity tends to flatten for these values,
 resembling a sigmoid curve.
 This is because at zero decay on a log scale would yield the maximum capacity
 
\begin_inset Formula $\left.P=2N\right.$
\end_inset

 and for higher decay 
\begin_inset Formula $\left.P\right.$
\end_inset

 is always greater than 0.
 We tested this for 
\begin_inset Formula $\left.N=250\right.$
\end_inset

 and is as shown in the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:sigmoid"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/patterns_250.png
	scale 50
	rotateOrigin center

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Maximum number of patterns trained by a perceptron for N=250 and for a wide
 range of decay values.
 Each data point is an average of 50 runs and the error bar is the standard
 deviation of the 50 runs.
\begin_inset CommandInset label
LatexCommand label
name "fig:sigmoid"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Performance of a perceptron
\end_layout

\begin_layout Standard
The performance of a perceptron is measured based on its ability to learn
 the input patterns and appropriately classify them.
 This measure, also called the accuracy of a perceptron, is defined as the
 number of input patterns that are correctly classified out of all the input
 patterns presented.
 A perceptron with a passive decay on its weights may not always learn all
 the patterns during the training process.
 Depending on the value of the decay rate, the training process may reach
 a point where there is no improvement in the accuracy of the perceptron.
 So, it becomes imperative to quit the training early instead of trying
 to train the perceptron over multiple epochs with no improvement in its
 accuracy.
 In order to achieve this, we devised a method to monitor the accuracy of
 the perceptron over the training process and quit it when the accuracy
 no longer improves.
 In this method, we calculate the accuracy of the perceptron in each epoch
 and quit the training when the mean accuracy of the last 
\begin_inset Formula $\left.(n+1)\right.$
\end_inset

 to 
\begin_inset Formula $\left.2n\right.$
\end_inset

 epochs exceeds mean accuracy of the last 
\begin_inset Formula $\left.n\right.$
\end_inset

 epochs.
 We calculated the optimal value of 
\begin_inset Formula $\left.n\right.$
\end_inset

, the window size, by training the perceptron with different values for
 
\begin_inset Formula $\left.n\right.$
\end_inset

 and analysing the value of accuracy with which the perceptron quits the
 training (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:window_size"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 We decided on the value of 
\begin_inset Formula $\left.n=25\right.$
\end_inset

 as the mean accuracy starts to plateau around that value.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/window_sizes.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Accuracy with which the perceptron quits for different 
\begin_inset Formula $\left.n\right.$
\end_inset

 where 
\begin_inset Formula $\left.n\right.$
\end_inset

 represents the window size.
 The value of decay was fixed to 
\begin_inset Formula $\left.1e^{-6}\right.$
\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "fig:window_size"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We then tested the behaviour of the perceptron by fixing the number of synapses
 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

 and the number of patterns to train as 
\begin_inset Formula $\left.1600\right.$
\end_inset

.
 We measured the accuracy, learning rate, and the energy consumed by the
 perceptron for a decay range of 
\begin_inset Formula $\left.10^{-8}\right.$
\end_inset

 to 
\begin_inset Formula $\left.10^{-2}\right.$
\end_inset

 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100line%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/accuracy.png
	scale 50

\end_inset


\begin_inset Graphics
	filename figures/epoch_accuracy.png
	scale 50

\end_inset


\end_layout

\end_inset


\begin_inset VSpace defskip
\end_inset


\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/error.png
	scale 50

\end_inset


\begin_inset Graphics
	filename figures/energy_accuracy.png
	scale 50

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
Performance of the perceptron: 
\series default
Different performance metrics along with energy consumed are measured for
 the perceptron with a decay in weights, resulting in passive forgetting.
 (Top left) Accuracy of the perceptron measured as the number of input patterns
 that are correctly classified among all the input patterns.
 (Top right) Epochs or the learning time of perceptron is the measure of
 the time (in epochs) required for the perceptron to learn input patterns
 until there is no further improvement in its accuracy.
 (Bottom left) Error rate of the perceptron after the input patterns are
 trained until there is no improvement in the accuracy.
 (Bottom right) Energy expended during the training process.
 The dashed line indicates the energy value required to train the perceptron
 when there is no decay.
 The results indicate the values obtained by training the perceptron with
 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

 and 
\begin_inset Formula $\left.1600\right.$
\end_inset

 patterns over a decay range of 
\begin_inset Formula $\left.10^{-8}\right.$
\end_inset

 to 
\begin_inset Formula $\left.10^{-2}\right.$
\end_inset

.
 Each data point is the average of 
\begin_inset Formula $\left.50\right.$
\end_inset

 runs.
 The uncertainty shown for each data point is the standard deviation of
 
\begin_inset Formula $\left.50\right.$
\end_inset

 runs.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Performance"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
From Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can see that with the increase in decay rate of the weights, the accuracy
 of the perceptron decreases and plateaus for higher decay values.
 A similar pattern can be observed for epochs or the learning time of the
 perceptron, where the learning time is high for low decay rates as the
 perceptron has a larger capacity, whereas as the decay value increases,
 the perceptron capacity reduces and so does the learning time, until it
 plateaus for high decay values.
 On the contrary, the curve showing the error in training the perceptron
 follows an opposite trend.
 The total error of the perceptron is zero at low decay values as the perceptron
 is able to train all the input patterns.
 While the decay value increases, the error value also increases due to
 the perceptron not being able to train all the patterns and the curve plateaus
 at high decay values.
\end_layout

\begin_layout Standard
However, the trend of the curve showing the energy required for the perceptron
 to train the input patterns for a range of decay values is not straightforward
 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

 bottom right).
 The energy expended by the perceptron to learn the patterns increases as
 the decay rate increases till the decay value reaches 
\begin_inset Formula $\left.10^{-5}\right.$
\end_inset

 after which the energy decreases till the decay value reaches 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

 after which the value of energy increases again.
 In order to gain more insight into this behaviour of energy consumed, we
 checked how the weights of the perceptron are updated in each epoch during
 the process of training as the value of total energy expended is directly
 determined by the change its synaptic weights (Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Thus, we plotted the number of updates to the synaptic weights versus each
 epoch in the training (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:epoch-updates"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 
\end_layout

\begin_layout Standard
Even though the number of updates to the weights is more for the decay value
 of 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

 (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:epoch-updates"
plural "false"
caps "false"
noprefix "false"

\end_inset

 left), the perceptron has trained for a fewer epochs when compared to the
 decay value of 
\begin_inset Formula $\left.10^{-5}\right.$
\end_inset

.
 So by the end of training, the perceptron has accumulated only a few synaptic
 weight updates in the former case and hence a dip in the energy curve for
 that decay value (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

 bottom-right).
 However, for the range of decay values in 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

 and 
\begin_inset Formula $\left.10^{-3}\right.$
\end_inset

, the energy expended by the perceptron increases again with the increase
 in decay.
 Clearly, the number of updates to the synaptic weights is more for decay
 value 
\begin_inset Formula $\left.10^{-3}\right.$
\end_inset

 when compared to the decay value of 
\begin_inset Formula $10^{-4}$
\end_inset

 even if there is not much of a difference in the number of epochs required
 for training (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:epoch-updates"
plural "false"
caps "false"
noprefix "false"

\end_inset

 right).
 Also, accuracy of the perceptron is higher for the latter case (Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

 top-left) accounting for fewer weight updates than the former.
 This explains why there in an increase in the energy curve in the decay
 range 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

 and 
\begin_inset Formula $\left.10^{-3}\right.$
\end_inset

.
\end_layout

\begin_layout Standard
Therefore, the energy required to train the perceptron increases as the
 decay increases because with the decay in weights the perceptron spends
 more time steps to learn the input patterns which leads to more updates
 in its weights thus leading to the increase in the total energy required
 for training.
 Although, for very low decay values, between 
\begin_inset Formula $\left.10^{-8}\right.$
\end_inset

 and 
\begin_inset Formula $\left.10^{-7}\right.$
\end_inset

, we can observe a small amount of energy being saved (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

 bottom-right) where the total number of updates to the weights is less
 than the scenario without decay.
 Hence, passive decay in weights during the training process contributes
 to energy conservation in the perceptron for very low decay values.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/epoch_updates_1.png
	scale 45

\end_inset


\begin_inset Graphics
	filename figures/epoch_updates_2.png
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The number of updates to the synaptic weights of the perceptron during the
 training process.
 (Left) The number of updates to weights over multiple runs for the decay
 values 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

, 
\begin_inset Formula $\left.10^{-5}\right.$
\end_inset

, and 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

.
 (Right) The number of updates to the synaptic weights over multiple runs
 for the decay values 
\begin_inset Formula $\left.10^{-4}\right.$
\end_inset

 and 
\begin_inset Formula $\left.10^{-3}\right.$
\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:epoch-updates"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Algorithms for forgetting
\end_layout

\begin_layout Standard
In the previous section, we studied the accuracy of the perceptron while
 learning the input patterns with passive forgetting.
 The results suggested that the training of a perceptron with passive forgetting
 did not show significant conservation in the energy consumed.
 In addition to that, we wanted to check if the perceptron saves energy
 in the process of forgetting and uses it for future learning of new patterns.
 In order to test this, we developed algorithms with either catastrophic
 or passive forgetting and train the perceptron to learn new patterns.
 We then compare the energy required to train the perceptron using each
 of the algorithms with a benchmark value of energy consumed to train a
 perceptron without any forgetting.
 
\end_layout

\begin_layout Standard
Firstly, a perceptron with 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

 synapses is considered and trained for 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns without any decay on the synaptic weights.
 The perceptron is then trained for the first 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns along with an additional 
\begin_inset Formula $\left.100\right.$
\end_inset

 patterns.
 The energy expended in training these patterns would be the 
\emph on
benchmark
\emph default
 value we use to compare with other algorithms.
 We developed three algorithms of training the perceptron with catastrophic
 forgetting and two algorithms with passive forgetting.
 We implement catastrophic forgetting as an interference by deliberately
 forgetting some of the old patterns learnt by the perceptron and introducing
 some new patterns to train.
 Passive forgetting is implemented similar to the previous section where
 the synaptic weights decay away at a constant rate after each time step
 of training the perceptron with an additional learning of new patterns.
 All the algorithms are implemented on a perceptron with 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

 synaptic connections.
 The number of patterns used in each algorithm is given in table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Number-of-patterns"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
The first algorithm is implemented by training the perceptron with 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns, then by training with a new set of 
\begin_inset Formula $\left.700\right.$
\end_inset

 randomly selected old patterns and 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns and measuring the energy required in training the new set.
 Note that here forgetting is implemented as intentional forgetting of some
 of the old patterns and interfering those by learning new patterns.
 The goal of this algorithm is to check the energy consumption having a
 new training set with total number of patterns (
\begin_inset Formula $\left.700+100=800)\right.$
\end_inset

 less than the initial training set (
\begin_inset Formula $\left.1000\right.$
\end_inset

).
 We label this algorithm as 
\emph on
catastrophic forgetting 1.
 
\emph default
For the second algorithm, we initially train the perceptron with 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns and then completely forget the previous 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns and only train with 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns the second time and measure the energy consumed.
 The objective here is to measure the energy consumed with a significantly
 lower number of patterns than the number used to train originally.
 We name this algorithm as 
\emph on
catastrophic forgetting 2.
 
\emph default
The third algorithm is implemented by initially training a perceptron with
 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns, then by training a new set of 
\begin_inset Formula $\left.900\right.$
\end_inset

 randomly selected old patterns and 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns, and measuring the energy expended in training the new set.
 The aim of this algorithm is to measure the energy consumption having a
 new training set 
\begin_inset Formula $\left.(900+100=1000)\right.$
\end_inset

 equal to the number of patterns in the original set 
\begin_inset Formula $\left.(1000)\right.$
\end_inset

.
 This is called as 
\emph on
catastrophic forgetting 3.
\end_layout

\begin_layout Standard
The third algorithm is implemented with active forgetting by first training
 the perceptron initially with 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns with decay on the synaptic weights, then training 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns along with the old patterns still retaining the decay on weights.
 We name this algorithm as 
\emph on
passive forgetting 1.
 
\emph default
In the last algorithm, we first train the perceptron with 
\begin_inset Formula $\left.1000\right.$
\end_inset

 patterns with decay on the synaptic weights, then train with a new set
 of 
\begin_inset Formula $\left.900\right.$
\end_inset

 randomly selected old patterns along with 
\begin_inset Formula $\left.100\right.$
\end_inset

 new patterns by retaining the decay on weights and measure the energy expended
 in training the new set.
 We name this algorithm as 
\emph on
passive forgetting 2.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align left
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="5">
<features tabularvalignment="middle">
<column alignment="center" valignment="middle" width="18text%">
<column alignment="center" valignment="middle" width="18text%">
<column alignment="center" valignment="middle" width="18text%">
<column alignment="center" valignment="middle" width="18text%">
<column alignment="center" valignment="middle" width="18text%">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
Algorithm
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
Initial training set
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
# of patterns forgotten
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
# of new patterns
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
Total patterns: new training set
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
Benchmark
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
1100
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
Catastrophic forgetting 1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
300
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
800
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
Catastrophic forgetting 2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
100
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
Catastrophic forgetting 3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
1000
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
Passive forgetting 1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
1100
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
Passive forgetting 2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
1000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
100
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
1000
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\align left
\begin_inset Caption Standard

\begin_layout Plain Layout
Number of patterns for training the perceptron with different algorithms.
 The number of synapses in the perceptron is set to 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "tab:Number-of-patterns"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align left
\begin_inset Graphics
	filename figures/energy_bar.png
	scale 42

\end_inset


\begin_inset Graphics
	filename figures/energy_forgetting_line.png
	scale 42

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
Forgetting in a perceptron and energy consumption: 
\series default
(Left) Energy consumed for all the five different algorithms of learning
 in a perceptron with forgetting (catastrophic or passive).
 The perceptron has 
\begin_inset Formula $\left.N=1000\right.$
\end_inset

 synapses and trained for 
\begin_inset Formula $\left.1000\right.$
\end_inset

 initial patterns.
 The decay value for passive forgetting is fixed to 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

.
 (Right) The energy consumption measured over a range of decay values from
 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

 to 
\begin_inset Formula $\left.5*10^{-6}\right.$
\end_inset

 for passive forgetting.
 The dashed lines represent the energy values for benchmark and three learning
 algorithms with catastrophic forgetting.
 The energy data points in both the graphs is an average value of 
\begin_inset Formula $\left.50\right.$
\end_inset

 runs.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Forgetting-energy"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We trained the perceptron with all the algorithms discussed above and plotted
 energy required to train the perceptron (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Forgetting-energy"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 First the perceptron was trained with a fixed decay value of 
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

 for passive forgetting algorithms.
 The results indicate that the energy is conserved for all the algorithms
 when compared to the benchmark value except for passive forgetting 1 (Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Forgetting-energy"
plural "false"
caps "false"
noprefix "false"

\end_inset

 (left)).
 We found similar results when we trained the perceptron over a range of
 decay values (
\begin_inset Formula $\left.10^{-6}\right.$
\end_inset

 to 
\begin_inset Formula $\left.5*10^{-6}\right.$
\end_inset

) for passive forgetting, figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Forgetting-energy"
plural "false"
caps "false"
noprefix "false"

\end_inset

 (right).
 
\end_layout

\begin_layout Standard
The total number of patterns in the new training set for catastrophic forgetting
 1 and catastrophic forgetting 2 algorithms is less than the number of patterns
 used for catastrophic forgetting 3 (Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Number-of-patterns"
plural "false"
caps "false"
noprefix "false"

\end_inset

 last column).
 All the catastrophic forgetting algorithms perform better with respect
 to conserving energy when compared to the benchmark (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Forgetting-energy"
plural "false"
caps "false"
noprefix "false"

\end_inset

 (left)).
 Catastrophic forgetting 2 conserves the most energy because the new training
 set contains only the new patterns with all the originally trained patterns
 forgotten.
 Thus the perceptron takes fewer epochs to quickly learn the new patterns
 which is far less than the patterns in the original set, thus spending
 less energy.
 The total number of patterns in catastrophic forgetting 1 algorithm is
 less than the number of patterns used in catastrophic forgetting 3.
 An already trained perceptron learns the new set of patterns in the former
 scenario because more patterns are forgotten as opposed to the latter.
 This allows the perceptron to learn the new patterns with fewer updates
 to the weights without any interference to the old patterns, consequently
 expending less energy.
 In case of passive forgetting algorithms, the total number of patterns
 used is less for passive forgetting 2 than passive forgetting 1.
 In passive forgetting 2, forgetting is a combination of active decay of
 synaptic weights and intentional forgetting of a few already learnt patterns.
 This combination gives the perceptron a little leeway to learn the new
 patterns more effectively than if it had learnt all at once (benchmark).
 On the contrary, in passive forgetting 1, only decay of weights don't contribut
e to energy conservation as the constant decay at each time step of training
 requires the perceptron to update more often, therefore spending more energy.
 We have seen a similar behaviour previously in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

 (bottom right).
\end_layout

\begin_layout Standard
We extended this analysis for a range of decay values for passive forgetting
 algorithms such that the perceptron trained all the patterns with 
\begin_inset Formula $\left.100\%\right.$
\end_inset

 accuracy.
 The catastrophic forgetting algorithms are not affected by this as there
 are not dependent on decay values, figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Forgetting-energy"
plural "false"
caps "false"
noprefix "false"

\end_inset

 (right).
 The passive forgetting 2 algorithm conserved energy and the amount of energy
 conserved decreased with increase in the decay value till 
\begin_inset Formula $\left.4.5*10^{-6}\right.$
\end_inset

 after which there was no energy gain.
 The passive forgetting 1 algorithm did not conserve energy for all the
 values of decay.
 Hence, the catastrophic forgetting algorithms performed well overall and
 conserved more energy as compared to passive forgetting algorithms.
\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
Synaptic plasticity is known to be the neural basis of learning in animals.
 The synapses strengthen during learning and leads to the formation of short-ter
m and long-term memories.
 Forgetting plays an integral part in the maintenance of these memories
 and helps the animals to adapt to varying environments.
 The formation of long-term memories through synaptic plasticity is an energy
 intensive process.
 We combined the concept of forgetting to investigate its role on energy
 consumption and to learn new memories.
 With the help of a perceptron, we implemented two types of forgetting,
 namely passive and catastrophic forgetting.
 Firstly, we implemented passive forgetting as a constant decay on the synaptic
 weights of the perceptron and studied the maximum capacity of a perceptron.
 We showed that for different synaptic connections, the maximum capacity
 of the perceptron decreased with increase in the decay value.
 We also showed that the energy required to train the perceptron to its
 maximum capacity with passive forgetting decreased with increase in the
 decay value.
 Even though, biologically, not all the synaptic connections of a neuron
 contribute to memories, the result signifies the limitations on the capacity
 of forgetful networks to hold memories.
\end_layout

\begin_layout Standard
Secondly, we devised a method to quit training the perceptron when there
 is no further increase in its mean accuracy.
 We measured the accuracy, learning time, error rate, and the energy consumed
 by the perceptron and showed that the accuracy and learning time decreased
 with increase in decay till it flattened for high decay values, where as
 the error rate and energy increased with the decay and plateaued at high
 decay values.
 We demonstrated that passive forgetting caused a tiny amount of energy
 conservation for very low decay values.
 
\end_layout

\begin_layout Standard
Finally, in order to investigate if the perceptron saved energy by forgetting
 old patterns and used that to learn new patterns, we used the catastrophic
 and passive forgetting mechanisms in the brain and implemented algorithms
 to train the perceptron.
 We demonstrated that catastrophic forgetting algorithms always performed
 better as for saving energy relative to passive forgetting algorithms.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\end_layout

\begin_layout Plain Layout


\backslash
bibname{References}
\end_layout

\end_inset


\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "references/perceptron,references/energy-efficient-synaptic-plasticity,references/overcoming-catastrophic-forgetting-in-neural-networks,references/catastrophic-forgetting,references/stability-plasticity,references/continual-learning-review,references/biology_of_forgetting,references/a-simple-weight-decay-can-improve-generalization,references/chaotic_nn,references/cajal,references/hebb,references/short-term,references/ltp,references/brea,references/persistence_and_transience,references/a-cost-of-long-term-memory-in-drosophila,references/Primo_BibTeX_Export,references/food-shortage,references/PKM"
options "apalike"

\end_inset


\end_layout

\end_body
\end_document
